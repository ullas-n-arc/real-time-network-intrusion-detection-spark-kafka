{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b59c826",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34beae2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Install PySpark (Colab only - skip if running locally)\n",
    "# !pip install pyspark -q\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer\n",
    "from pyspark.ml.classification import RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import gc\n",
    "\n",
    "print(\"âœ… Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86280d54",
   "metadata": {},
   "source": [
    "## Step 2: Configure Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "360b8bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Running locally\n",
      "ðŸ“‚ Training data: /workspaces/real-time-network-intrusion-detection-spark-kafka/data/training/UNSW_NB15_training-set.csv\n",
      "ðŸ“‚ Testing data: /workspaces/real-time-network-intrusion-detection-spark-kafka/data/testing/UNSW_NB15_testing-set.csv\n",
      "ðŸ“‚ Model directory: /workspaces/real-time-network-intrusion-detection-spark-kafka/models\n"
     ]
    }
   ],
   "source": [
    "# Configure paths based on environment\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    BASE_DIR = \"/content/drive/MyDrive/NetworkIDS\"\n",
    "    TRAIN_PATH = f\"{BASE_DIR}/data/training/UNSW_NB15_training-set.csv\"\n",
    "    TEST_PATH = f\"{BASE_DIR}/data/testing/UNSW_NB15_testing-set.csv\"\n",
    "    MODEL_DIR = f\"{BASE_DIR}/models\"\n",
    "    IS_COLAB = True\n",
    "    print(f\"âœ… Google Drive mounted successfully!\")\n",
    "except:\n",
    "    BASE_DIR = \"/workspaces/real-time-network-intrusion-detection-spark-kafka\"\n",
    "    TRAIN_PATH = f\"{BASE_DIR}/data/training/UNSW_NB15_training-set.csv\"\n",
    "    TEST_PATH = f\"{BASE_DIR}/data/testing/UNSW_NB15_testing-set.csv\"\n",
    "    MODEL_DIR = f\"{BASE_DIR}/models\"\n",
    "    IS_COLAB = False\n",
    "    print(f\"âœ… Running locally\")\n",
    "\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"ðŸ“‚ Training data: {TRAIN_PATH}\")\n",
    "print(f\"ðŸ“‚ Testing data: {TEST_PATH}\")\n",
    "print(f\"ðŸ“‚ Model directory: {MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47fc4c8",
   "metadata": {},
   "source": [
    "## Step 3: Create Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2eefa8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/06 12:51:15 WARN Utils: Your hostname, codespaces-e7653b resolves to a loopback address: 127.0.0.1; using 10.0.1.39 instead (on interface eth0)\n",
      "25/12/06 12:51:15 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/06 12:51:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/12/06 12:51:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Spark session created\n",
      "ðŸ“Š Spark version: 3.5.7\n"
     ]
    }
   ],
   "source": [
    "# Force garbage collection before creating session\n",
    "gc.collect()\n",
    "\n",
    "# Create Spark session optimized for ML training\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"UNSW-NB15-ModelTraining\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"50\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "    .config(\"spark.network.timeout\", \"800s\") \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"60s\") \\\n",
    "    .config(\"spark.sql.broadcastTimeout\", \"600\") \\\n",
    "    .config(\"spark.memory.fraction\", \"0.6\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "print(f\"âœ… Spark session created\")\n",
    "print(f\"ðŸ“Š Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a88715",
   "metadata": {},
   "source": [
    "## Step 4: Load UNSW-NB15 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17970cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading UNSW-NB15 dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dataset loaded in 10.22 seconds\n",
      "ðŸ“Š Training records: 73,408\n",
      "ðŸ“Š Training records: 73,408\n",
      "ðŸ“Š Testing records: 82,332\n",
      "ðŸ“Š Total features: 45\n",
      "ðŸ“Š Testing records: 82,332\n",
      "ðŸ“Š Total features: 45\n"
     ]
    }
   ],
   "source": [
    "# Load training and testing data\n",
    "print(\"Loading UNSW-NB15 dataset...\")\n",
    "start_time = time.time()\n",
    "\n",
    "train_df = spark.read.csv(TRAIN_PATH, header=True, inferSchema=True)\n",
    "test_df = spark.read.csv(TEST_PATH, header=True, inferSchema=True)\n",
    "\n",
    "print(f\"âœ… Dataset loaded in {time.time() - start_time:.2f} seconds\")\n",
    "print(f\"ðŸ“Š Training records: {train_df.count():,}\")\n",
    "print(f\"ðŸ“Š Testing records: {test_df.count():,}\")\n",
    "print(f\"ðŸ“Š Total features: {len(train_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff0e1605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Schema (first 15 columns):\n",
      "  id: IntegerType()\n",
      "  dur: DoubleType()\n",
      "  proto: StringType()\n",
      "  service: StringType()\n",
      "  state: StringType()\n",
      "  spkts: IntegerType()\n",
      "  dpkts: IntegerType()\n",
      "  sbytes: IntegerType()\n",
      "  dbytes: IntegerType()\n",
      "  rate: DoubleType()\n",
      "  sttl: IntegerType()\n",
      "  dttl: IntegerType()\n",
      "  sload: DoubleType()\n",
      "  dload: DoubleType()\n",
      "  sloss: IntegerType()\n"
     ]
    }
   ],
   "source": [
    "# Show schema\n",
    "print(\"Dataset Schema (first 15 columns):\")\n",
    "for i, field in enumerate(train_df.schema.fields[:15]):\n",
    "    print(f\"  {field.name}: {field.dataType}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e3ee939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Attack Category Distribution (Training):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|    attack_cat|count|\n",
      "+--------------+-----+\n",
      "|        Normal|49133|\n",
      "|      Exploits|10340|\n",
      "|       Fuzzers| 6133|\n",
      "|           DoS| 3264|\n",
      "|Reconnaissance| 2930|\n",
      "|      Analysis|  693|\n",
      "|      Backdoor|  540|\n",
      "|     Shellcode|  333|\n",
      "|         Worms|   41|\n",
      "|          NULL|    1|\n",
      "+--------------+-----+\n",
      "\n",
      "\n",
      "ðŸ“Š Binary Label Distribution (Training):\n",
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    0|49133|\n",
      "| NULL|    1|\n",
      "|    1|24274|\n",
      "+-----+-----+\n",
      "\n",
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    0|49133|\n",
      "| NULL|    1|\n",
      "|    1|24274|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check label distribution\n",
    "print(\"\\nðŸ“Š Attack Category Distribution (Training):\")\n",
    "train_df.groupBy(\"attack_cat\").count().orderBy(F.desc(\"count\")).show()\n",
    "\n",
    "print(\"\\nðŸ“Š Binary Label Distribution (Training):\")\n",
    "train_df.groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d370324",
   "metadata": {},
   "source": [
    "## Step 5: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c22655e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Using 39 numeric features\n",
      "ðŸ“Š 11 attack categories\n"
     ]
    }
   ],
   "source": [
    "# Define feature columns (exclude id, label columns, and categorical)\n",
    "NUMERIC_FEATURES = [\n",
    "    \"dur\", \"spkts\", \"dpkts\", \"sbytes\", \"dbytes\", \"rate\", \"sttl\", \"dttl\",\n",
    "    \"sload\", \"dload\", \"sloss\", \"dloss\", \"sinpkt\", \"dinpkt\", \"sjit\", \"djit\",\n",
    "    \"swin\", \"stcpb\", \"dtcpb\", \"dwin\", \"tcprtt\", \"synack\", \"ackdat\",\n",
    "    \"smean\", \"dmean\", \"trans_depth\", \"response_body_len\", \"ct_srv_src\",\n",
    "    \"ct_state_ttl\", \"ct_dst_ltm\", \"ct_src_dport_ltm\", \"ct_dst_sport_ltm\",\n",
    "    \"ct_dst_src_ltm\", \"is_ftp_login\", \"ct_ftp_cmd\", \"ct_flw_http_mthd\",\n",
    "    \"ct_src_ltm\", \"ct_srv_dst\", \"is_sm_ips_ports\"\n",
    "]\n",
    "\n",
    "# Attack category mapping\n",
    "ATTACK_MAPPING = {\n",
    "    \"Normal\": 0,\n",
    "    \"Fuzzers\": 1,\n",
    "    \"Analysis\": 2,\n",
    "    \"Backdoors\": 3,\n",
    "    \"Backdoor\": 3,\n",
    "    \"DoS\": 4,\n",
    "    \"Exploits\": 5,\n",
    "    \"Generic\": 6,\n",
    "    \"Reconnaissance\": 7,\n",
    "    \"Shellcode\": 8,\n",
    "    \"Worms\": 9\n",
    "}\n",
    "\n",
    "print(f\"ðŸ“Š Using {len(NUMERIC_FEATURES)} numeric features\")\n",
    "print(f\"ðŸ“Š {len(ATTACK_MAPPING)} attack categories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7040bb74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing training data...\n",
      "Preprocessing testing data...\n",
      "Preprocessing testing data...\n",
      "âœ… Data preprocessing complete\n",
      "âœ… Data preprocessing complete\n"
     ]
    }
   ],
   "source": [
    "def preprocess_unsw_data(df):\n",
    "    \"\"\"Preprocess UNSW-NB15 data: clean, handle nulls, create labels\"\"\"\n",
    "    \n",
    "    # Clean attack_cat column\n",
    "    df = df.withColumn(\"attack_cat\", F.trim(F.col(\"attack_cat\")))\n",
    "    df = df.withColumn(\"attack_cat\", \n",
    "        F.when(F.col(\"attack_cat\").isNull() | (F.col(\"attack_cat\") == \"\"), \"Normal\")\n",
    "        .otherwise(F.col(\"attack_cat\")))\n",
    "    \n",
    "    # Create binary label (0 = Normal, 1 = Attack)\n",
    "    df = df.withColumn(\"binary_label\",\n",
    "        F.when(F.col(\"attack_cat\") == \"Normal\", 0.0).otherwise(1.0))\n",
    "    \n",
    "    # Create multiclass label from mapping\n",
    "    label_expr = F.lit(0)  # Default to Normal\n",
    "    for attack, label in ATTACK_MAPPING.items():\n",
    "        label_expr = F.when(F.col(\"attack_cat\") == attack, label).otherwise(label_expr)\n",
    "    df = df.withColumn(\"multiclass_label\", label_expr.cast(DoubleType()))\n",
    "    \n",
    "    # Handle null/infinite values in numeric features\n",
    "    for col in NUMERIC_FEATURES:\n",
    "        if col in df.columns:\n",
    "            df = df.withColumn(col,\n",
    "                F.when(F.col(col).isNull(), 0.0)\n",
    "                .when(F.col(col) == float(\"inf\"), 0.0)\n",
    "                .when(F.col(col) == float(\"-inf\"), 0.0)\n",
    "                .otherwise(F.col(col).cast(DoubleType())))\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"Preprocessing training data...\")\n",
    "train_df = preprocess_unsw_data(train_df)\n",
    "\n",
    "print(\"Preprocessing testing data...\")\n",
    "test_df = preprocess_unsw_data(test_df)\n",
    "\n",
    "print(\"âœ… Data preprocessing complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "310929c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Binary Label Distribution (Training):\n",
      "+------------+-----+\n",
      "|binary_label|count|\n",
      "+------------+-----+\n",
      "|         0.0|49134|\n",
      "|         1.0|24274|\n",
      "+------------+-----+\n",
      "\n",
      "\n",
      "ðŸ“Š Multiclass Label Distribution (Training):\n",
      "+------------+-----+\n",
      "|binary_label|count|\n",
      "+------------+-----+\n",
      "|         0.0|49134|\n",
      "|         1.0|24274|\n",
      "+------------+-----+\n",
      "\n",
      "\n",
      "ðŸ“Š Multiclass Label Distribution (Training):\n",
      "+----------------+--------------+-----+\n",
      "|multiclass_label|    attack_cat|count|\n",
      "+----------------+--------------+-----+\n",
      "|             0.0|        Normal|49134|\n",
      "|             1.0|       Fuzzers| 6133|\n",
      "|             2.0|      Analysis|  693|\n",
      "|             3.0|      Backdoor|  540|\n",
      "|             4.0|           DoS| 3264|\n",
      "|             5.0|      Exploits|10340|\n",
      "|             7.0|Reconnaissance| 2930|\n",
      "|             8.0|     Shellcode|  333|\n",
      "|             9.0|         Worms|   41|\n",
      "+----------------+--------------+-----+\n",
      "\n",
      "+----------------+--------------+-----+\n",
      "|multiclass_label|    attack_cat|count|\n",
      "+----------------+--------------+-----+\n",
      "|             0.0|        Normal|49134|\n",
      "|             1.0|       Fuzzers| 6133|\n",
      "|             2.0|      Analysis|  693|\n",
      "|             3.0|      Backdoor|  540|\n",
      "|             4.0|           DoS| 3264|\n",
      "|             5.0|      Exploits|10340|\n",
      "|             7.0|Reconnaissance| 2930|\n",
      "|             8.0|     Shellcode|  333|\n",
      "|             9.0|         Worms|   41|\n",
      "+----------------+--------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Verify label distributions after preprocessing\n",
    "print(\"\\nðŸ“Š Binary Label Distribution (Training):\")\n",
    "train_df.groupBy(\"binary_label\").count().show()\n",
    "\n",
    "print(\"\\nðŸ“Š Multiclass Label Distribution (Training):\")\n",
    "train_df.groupBy(\"multiclass_label\", \"attack_cat\").count().orderBy(\"multiclass_label\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd6720c",
   "metadata": {},
   "source": [
    "## Step 6: Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c2c7d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Available features: 39/39\n",
      "Features: ['dur', 'spkts', 'dpkts', 'sbytes', 'dbytes', 'rate', 'sttl', 'dttl', 'sload', 'dload', 'sloss', 'dloss', 'sinpkt', 'dinpkt', 'sjit', 'djit', 'swin', 'stcpb', 'dtcpb', 'dwin', 'tcprtt', 'synack', 'ackdat', 'smean', 'dmean', 'trans_depth', 'response_body_len', 'ct_srv_src', 'ct_state_ttl', 'ct_dst_ltm', 'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm', 'is_ftp_login', 'ct_ftp_cmd', 'ct_flw_http_mthd', 'ct_src_ltm', 'ct_srv_dst', 'is_sm_ips_ports']\n"
     ]
    }
   ],
   "source": [
    "# Get available features (some may be missing)\n",
    "available_features = [col for col in NUMERIC_FEATURES if col in train_df.columns]\n",
    "print(f\"ðŸ“Š Available features: {len(available_features)}/{len(NUMERIC_FEATURES)}\")\n",
    "print(f\"Features: {available_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7492095c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”§ Assembling features...\n",
      "âœ… Feature vectors assembled\n",
      "âœ… Feature vectors assembled\n"
     ]
    }
   ],
   "source": [
    "# Assemble features into vector\n",
    "print(\"\\nðŸ”§ Assembling features...\")\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=available_features,\n",
    "    outputCol=\"features_raw\",\n",
    "    handleInvalid=\"skip\"\n",
    ")\n",
    "\n",
    "train_df = assembler.transform(train_df)\n",
    "test_df = assembler.transform(test_df)\n",
    "\n",
    "print(\"âœ… Feature vectors assembled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b61e396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”§ Scaling features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Features scaled\n"
     ]
    }
   ],
   "source": [
    "# Scale features using StandardScaler\n",
    "print(\"\\nðŸ”§ Scaling features...\")\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features_raw\",\n",
    "    outputCol=\"features_scaled\",\n",
    "    withMean=True,\n",
    "    withStd=True\n",
    ")\n",
    "\n",
    "# Fit scaler on training data only\n",
    "scaler_model = scaler.fit(train_df)\n",
    "\n",
    "# Transform both train and test\n",
    "train_df = scaler_model.transform(train_df)\n",
    "test_df = scaler_model.transform(test_df)\n",
    "\n",
    "print(\"âœ… Features scaled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c76ef24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 28:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scaler saved to: /workspaces/real-time-network-intrusion-detection-spark-kafka/models/unsw_scaler\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Save the scaler model\n",
    "scaler_path = f\"{MODEL_DIR}/unsw_scaler\"\n",
    "scaler_model.write().overwrite().save(scaler_path)\n",
    "print(f\"âœ… Scaler saved to: {scaler_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f8db4d",
   "metadata": {},
   "source": [
    "## Step 7: Calculate Class Weights for Imbalanced Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a82465a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Calculating class weights for binary classification...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 29:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Class 0: 49,134 samples -> weight 0.8643\n",
      "  Class 1: 24,274 samples -> weight 1.2297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Calculate class weights for binary classification\n",
    "print(\"ðŸ“Š Calculating class weights for binary classification...\")\n",
    "binary_counts = train_df.groupBy(\"binary_label\").count().collect()\n",
    "total_binary = sum(row['count'] for row in binary_counts)\n",
    "\n",
    "binary_weights = {}\n",
    "for row in binary_counts:\n",
    "    label = row['binary_label']\n",
    "    count = row['count']\n",
    "    # Inverse frequency weighting with sqrt smoothing\n",
    "    weight = (total_binary / (2 * count)) ** 0.5\n",
    "    binary_weights[label] = weight\n",
    "    print(f\"  Class {int(label)}: {count:,} samples -> weight {weight:.4f}\")\n",
    "\n",
    "# Apply binary weights\n",
    "binary_weight_expr = F.lit(1.0)\n",
    "for label, weight in binary_weights.items():\n",
    "    binary_weight_expr = F.when(F.col(\"binary_label\") == label, weight).otherwise(binary_weight_expr)\n",
    "\n",
    "train_df = train_df.withColumn(\"binary_weight\", binary_weight_expr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d89aec9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Calculating class weights for multiclass classification...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 32:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Class 0 (Normal): 49,134 samples -> weight 0.4074\n",
      "  Class 8 (Shellcode): 333 samples -> weight 4.9491\n",
      "  Class 1 (Fuzzers): 6,133 samples -> weight 1.1532\n",
      "  Class 7 (Reconnaissance): 2,930 samples -> weight 1.6685\n",
      "  Class 5 (Exploits): 10,340 samples -> weight 0.8882\n",
      "  Class 4 (DoS): 3,264 samples -> weight 1.5808\n",
      "  Class 3 (Backdoors): 540 samples -> weight 3.8865\n",
      "  Class 9 (Worms): 41 samples -> weight 14.1045\n",
      "  Class 2 (Analysis): 693 samples -> weight 3.4307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Calculate class weights for multiclass classification\n",
    "print(\"\\nðŸ“Š Calculating class weights for multiclass classification...\")\n",
    "multi_counts = train_df.groupBy(\"multiclass_label\").count().collect()\n",
    "total_multi = sum(row['count'] for row in multi_counts)\n",
    "num_classes = len(multi_counts)\n",
    "\n",
    "multi_weights = {}\n",
    "for row in multi_counts:\n",
    "    label = row['multiclass_label']\n",
    "    count = row['count']\n",
    "    # Inverse frequency weighting with sqrt smoothing\n",
    "    weight = (total_multi / (num_classes * count)) ** 0.5\n",
    "    multi_weights[label] = weight\n",
    "    attack_name = [k for k, v in ATTACK_MAPPING.items() if v == int(label)][0] if label in range(10) else \"Unknown\"\n",
    "    print(f\"  Class {int(label)} ({attack_name}): {count:,} samples -> weight {weight:.4f}\")\n",
    "\n",
    "# Apply multiclass weights\n",
    "multi_weight_expr = F.lit(1.0)\n",
    "for label, weight in multi_weights.items():\n",
    "    multi_weight_expr = F.when(F.col(\"multiclass_label\") == label, weight).otherwise(multi_weight_expr)\n",
    "\n",
    "train_df = train_df.withColumn(\"multiclass_weight\", multi_weight_expr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48fe17ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ—ƒï¸ Caching prepared data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 39:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Training set: 73,408 records\n",
      "âœ… Test set: 82,332 records\n",
      "âœ… Data ready for training!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Cache prepared data\n",
    "print(\"\\nðŸ—ƒï¸ Caching prepared data...\")\n",
    "train_df = train_df.select(\n",
    "    \"features_scaled\", \"binary_label\", \"multiclass_label\", \n",
    "    \"binary_weight\", \"multiclass_weight\", \"attack_cat\"\n",
    ").cache()\n",
    "\n",
    "test_df = test_df.select(\n",
    "    \"features_scaled\", \"binary_label\", \"multiclass_label\", \"attack_cat\"\n",
    ").cache()\n",
    "\n",
    "# Materialize cache\n",
    "train_count = train_df.count()\n",
    "test_count = test_df.count()\n",
    "\n",
    "print(f\"âœ… Training set: {train_count:,} records\")\n",
    "print(f\"âœ… Test set: {test_count:,} records\")\n",
    "print(\"âœ… Data ready for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61fb639",
   "metadata": {},
   "source": [
    "## Step 8: Train Binary Classification Models\n",
    "\n",
    "### 8.1 Random Forest - Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c7a023b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Training Random Forest - Binary Classification\n",
      "============================================================\n",
      "ðŸš€ Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Training completed in 0.88 minutes\n"
     ]
    }
   ],
   "source": [
    "# Random Forest for Binary Classification\n",
    "print(\"=\"*60)\n",
    "print(\"Training Random Forest - Binary Classification\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "rf_binary = RandomForestClassifier(\n",
    "    featuresCol='features_scaled',\n",
    "    labelCol='binary_label',\n",
    "    weightCol='binary_weight',\n",
    "    numTrees=100,\n",
    "    maxDepth=15,\n",
    "    maxBins=64,\n",
    "    minInstancesPerNode=5,\n",
    "    featureSubsetStrategy='sqrt',\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"ðŸš€ Training model...\")\n",
    "rf_binary_model = rf_binary.fit(train_df)\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"âœ… Training completed in {elapsed/60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81b9534a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“ˆ Evaluating Random Forest - Binary Classification...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Random Forest - Binary Classification Results\n",
      "==================================================\n",
      "AUC-ROC:   0.9048\n",
      "AUC-PR:    0.9182\n",
      "Accuracy:  0.7908\n",
      "F1 Score:  0.7907\n",
      "Precision: 0.7906\n",
      "Recall:    0.7908\n",
      "\n",
      "ðŸ“Š Confusion Matrix:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 113:============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+-----+\n",
      "|binary_label|prediction|count|\n",
      "+------------+----------+-----+\n",
      "|         0.0|       0.0|28138|\n",
      "|         0.0|       1.0| 8862|\n",
      "|         1.0|       0.0| 8362|\n",
      "|         1.0|       1.0|36970|\n",
      "+------------+----------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "464"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate Random Forest - Binary\n",
    "print(\"\\nðŸ“ˆ Evaluating Random Forest - Binary Classification...\")\n",
    "\n",
    "rf_binary_preds = rf_binary_model.transform(test_df)\n",
    "\n",
    "# Binary metrics\n",
    "binary_evaluator_auc = BinaryClassificationEvaluator(\n",
    "    labelCol='binary_label',\n",
    "    rawPredictionCol='rawPrediction',\n",
    "    metricName='areaUnderROC'\n",
    ")\n",
    "\n",
    "binary_evaluator_pr = BinaryClassificationEvaluator(\n",
    "    labelCol='binary_label',\n",
    "    rawPredictionCol='rawPrediction',\n",
    "    metricName='areaUnderPR'\n",
    ")\n",
    "\n",
    "multi_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol='binary_label',\n",
    "    predictionCol='prediction'\n",
    ")\n",
    "\n",
    "auc_roc = binary_evaluator_auc.evaluate(rf_binary_preds)\n",
    "auc_pr = binary_evaluator_pr.evaluate(rf_binary_preds)\n",
    "accuracy = multi_evaluator.evaluate(rf_binary_preds, {multi_evaluator.metricName: 'accuracy'})\n",
    "f1 = multi_evaluator.evaluate(rf_binary_preds, {multi_evaluator.metricName: 'f1'})\n",
    "precision = multi_evaluator.evaluate(rf_binary_preds, {multi_evaluator.metricName: 'weightedPrecision'})\n",
    "recall = multi_evaluator.evaluate(rf_binary_preds, {multi_evaluator.metricName: 'weightedRecall'})\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Random Forest - Binary Classification Results\")\n",
    "print(\"=\"*50)\n",
    "print(f\"AUC-ROC:   {auc_roc:.4f}\")\n",
    "print(f\"AUC-PR:    {auc_pr:.4f}\")\n",
    "print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"F1 Score:  {f1:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "\n",
    "rf_binary_results = {\n",
    "    'model': 'Random Forest',\n",
    "    'task': 'Binary Classification',\n",
    "    'auc_roc': auc_roc,\n",
    "    'auc_pr': auc_pr,\n",
    "    'accuracy': accuracy,\n",
    "    'f1': f1,\n",
    "    'precision': precision,\n",
    "    'recall': recall\n",
    "}\n",
    "\n",
    "# Show confusion matrix\n",
    "print(\"\\nðŸ“Š Confusion Matrix:\")\n",
    "rf_binary_preds.groupBy(\"binary_label\", \"prediction\").count().orderBy(\"binary_label\", \"prediction\").show()\n",
    "\n",
    "rf_binary_preds.unpersist()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2738c153",
   "metadata": {},
   "source": [
    "### 8.2 Gradient Boosted Trees - Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d22f96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Training Gradient Boosted Trees - Binary Classification\n",
      "============================================================\n",
      "ðŸš€ Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Training completed in 0.68 minutes\n"
     ]
    }
   ],
   "source": [
    "# GBT for Binary Classification\n",
    "print(\"=\"*60)\n",
    "print(\"Training Gradient Boosted Trees - Binary Classification\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "gc.collect()\n",
    "start_time = time.time()\n",
    "\n",
    "gbt_binary = GBTClassifier(\n",
    "    featuresCol='features_scaled',\n",
    "    labelCol='binary_label',\n",
    "    weightCol='binary_weight',\n",
    "    maxIter=50,\n",
    "    maxDepth=8,\n",
    "    stepSize=0.1,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"ðŸš€ Training model...\")\n",
    "gbt_binary_model = gbt_binary.fit(train_df)\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"âœ… Training completed in {elapsed/60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b515a666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“ˆ Evaluating GBT - Binary Classification...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "GBT - Binary Classification Results\n",
      "==================================================\n",
      "AUC-ROC:   0.9431\n",
      "AUC-PR:    0.9504\n",
      "Accuracy:  0.8713\n",
      "F1 Score:  0.8696\n",
      "Precision: 0.8781\n",
      "Recall:    0.8713\n",
      "\n",
      "ðŸ“Š Confusion Matrix:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+-----+\n",
      "|binary_label|prediction|count|\n",
      "+------------+----------+-----+\n",
      "|         0.0|       0.0|28631|\n",
      "|         0.0|       1.0| 8369|\n",
      "|         1.0|       0.0| 2224|\n",
      "|         1.0|       1.0|43108|\n",
      "+------------+----------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "729"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate GBT - Binary\n",
    "print(\"\\nðŸ“ˆ Evaluating GBT - Binary Classification...\")\n",
    "\n",
    "gbt_binary_preds = gbt_binary_model.transform(test_df)\n",
    "\n",
    "auc_roc = binary_evaluator_auc.evaluate(gbt_binary_preds)\n",
    "auc_pr = binary_evaluator_pr.evaluate(gbt_binary_preds)\n",
    "accuracy = multi_evaluator.evaluate(gbt_binary_preds, {multi_evaluator.metricName: 'accuracy'})\n",
    "f1 = multi_evaluator.evaluate(gbt_binary_preds, {multi_evaluator.metricName: 'f1'})\n",
    "precision = multi_evaluator.evaluate(gbt_binary_preds, {multi_evaluator.metricName: 'weightedPrecision'})\n",
    "recall = multi_evaluator.evaluate(gbt_binary_preds, {multi_evaluator.metricName: 'weightedRecall'})\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"GBT - Binary Classification Results\")\n",
    "print(\"=\"*50)\n",
    "print(f\"AUC-ROC:   {auc_roc:.4f}\")\n",
    "print(f\"AUC-PR:    {auc_pr:.4f}\")\n",
    "print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"F1 Score:  {f1:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "\n",
    "gbt_binary_results = {\n",
    "    'model': 'Gradient Boosted Trees',\n",
    "    'task': 'Binary Classification',\n",
    "    'auc_roc': auc_roc,\n",
    "    'auc_pr': auc_pr,\n",
    "    'accuracy': accuracy,\n",
    "    'f1': f1,\n",
    "    'precision': precision,\n",
    "    'recall': recall\n",
    "}\n",
    "\n",
    "# Show confusion matrix\n",
    "print(\"\\nðŸ“Š Confusion Matrix:\")\n",
    "gbt_binary_preds.groupBy(\"binary_label\", \"prediction\").count().orderBy(\"binary_label\", \"prediction\").show()\n",
    "\n",
    "gbt_binary_preds.unpersist()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a02dfad",
   "metadata": {},
   "source": [
    "## Step 9: Train Multi-class Classification Model\n",
    "\n",
    "### 9.1 Random Forest - Multi-class (10 attack categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ca0c704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Training Random Forest - Multi-class Classification (10 classes)\n",
      "============================================================\n",
      "ðŸš€ Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Training completed in 1.66 minutes\n"
     ]
    }
   ],
   "source": [
    "# Random Forest for Multi-class Classification\n",
    "print(\"=\"*60)\n",
    "print(\"Training Random Forest - Multi-class Classification (10 classes)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "gc.collect()\n",
    "start_time = time.time()\n",
    "\n",
    "rf_multi = RandomForestClassifier(\n",
    "    featuresCol='features_scaled',\n",
    "    labelCol='multiclass_label',\n",
    "    weightCol='multiclass_weight',\n",
    "    numTrees=100,\n",
    "    maxDepth=15,\n",
    "    maxBins=64,\n",
    "    minInstancesPerNode=3,\n",
    "    featureSubsetStrategy='sqrt',\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"ðŸš€ Training model...\")\n",
    "rf_multi_model = rf_multi.fit(train_df)\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"âœ… Training completed in {elapsed/60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c0e1ddaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“ˆ Evaluating Random Forest - Multi-class Classification...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1019:============================>                           (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Random Forest - Multi-class Classification Results\n",
      "==================================================\n",
      "Accuracy:  0.5088\n",
      "F1 Score:  0.4852\n",
      "Precision: 0.4867\n",
      "Recall:    0.5088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Evaluate Random Forest - Multi-class\n",
    "print(\"\\nðŸ“ˆ Evaluating Random Forest - Multi-class Classification...\")\n",
    "\n",
    "rf_multi_preds = rf_multi_model.transform(test_df)\n",
    "\n",
    "mc_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol='multiclass_label',\n",
    "    predictionCol='prediction'\n",
    ")\n",
    "\n",
    "accuracy = mc_evaluator.evaluate(rf_multi_preds, {mc_evaluator.metricName: 'accuracy'})\n",
    "f1 = mc_evaluator.evaluate(rf_multi_preds, {mc_evaluator.metricName: 'f1'})\n",
    "precision = mc_evaluator.evaluate(rf_multi_preds, {mc_evaluator.metricName: 'weightedPrecision'})\n",
    "recall = mc_evaluator.evaluate(rf_multi_preds, {mc_evaluator.metricName: 'weightedRecall'})\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Random Forest - Multi-class Classification Results\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"F1 Score:  {f1:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "\n",
    "rf_multi_results = {\n",
    "    'model': 'Random Forest',\n",
    "    'task': 'Multi-class Classification (10 classes)',\n",
    "    'accuracy': accuracy,\n",
    "    'f1': f1,\n",
    "    'precision': precision,\n",
    "    'recall': recall\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7fb3ddb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Per-Class Performance:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+-------+--------+\n",
      "|multiclass_label|total|correct|accuracy|\n",
      "+----------------+-----+-------+--------+\n",
      "|             0.0|37000|  25784|  0.6969|\n",
      "|             1.0| 6062|   3575|  0.5897|\n",
      "|             2.0|  677|     30|  0.0443|\n",
      "|             3.0|  583|     83|  0.1424|\n",
      "|             4.0| 4089|    922|  0.2255|\n",
      "|             5.0|11132|   8318|  0.7472|\n",
      "|             6.0|18871|      0|     0.0|\n",
      "|             7.0| 3496|   2891|  0.8269|\n",
      "|             8.0|  378|    280|  0.7407|\n",
      "|             9.0|   44|      4|  0.0909|\n",
      "+----------------+-----+-------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Class Attack Type            Accuracy   Correct/Total\n",
      "------------------------------------------------------------\n",
      "0     Normal                   69.69%  25784/37000   \n",
      "1     Fuzzers                  58.97%   3575/6062    \n",
      "2     Analysis                  4.43%     30/677     \n",
      "3     Backdoor                 14.24%     83/583     \n",
      "4     DoS                      22.55%    922/4089    \n",
      "5     Exploits                 74.72%   8318/11132   \n",
      "6     Generic                   0.00%      0/18871   \n",
      "7     Reconnaissance           82.69%   2891/3496    \n",
      "8     Shellcode                74.07%    280/378     \n",
      "9     Worms                     9.09%      4/44      \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "225"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Per-class accuracy analysis\n",
    "print(\"\\nðŸ“Š Per-Class Performance:\")\n",
    "\n",
    "# Create reverse mapping\n",
    "reverse_mapping = {v: k for k, v in ATTACK_MAPPING.items()}\n",
    "\n",
    "per_class_stats = rf_multi_preds.withColumn(\n",
    "    'correct', F.when(F.col('multiclass_label') == F.col('prediction'), 1).otherwise(0)\n",
    ").groupBy('multiclass_label').agg(\n",
    "    F.count('*').alias('total'),\n",
    "    F.sum('correct').alias('correct'),\n",
    "    F.round(F.sum('correct') / F.count('*'), 4).alias('accuracy')\n",
    ").orderBy('multiclass_label')\n",
    "\n",
    "per_class_stats.show()\n",
    "\n",
    "# Collect for detailed printing\n",
    "stats = per_class_stats.collect()\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(f\"{'Class':<5} {'Attack Type':<20} {'Accuracy':>10} {'Correct/Total':>15}\")\n",
    "print(\"-\"*60)\n",
    "for row in stats:\n",
    "    label = int(row['multiclass_label'])\n",
    "    attack_name = reverse_mapping.get(label, \"Unknown\")\n",
    "    print(f\"{label:<5} {attack_name:<20} {row['accuracy']*100:>9.2f}% {row['correct']:>6}/{row['total']:<8}\")\n",
    "\n",
    "rf_multi_preds.unpersist()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2afb371",
   "metadata": {},
   "source": [
    "## Step 10: Save Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0de79abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Saving trained models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved: /workspaces/real-time-network-intrusion-detection-spark-kafka/models/unsw_rf_binary_classifier\n",
      "âœ… Saved: /workspaces/real-time-network-intrusion-detection-spark-kafka/models/unsw_gbt_binary_classifier\n",
      "âœ… Saved: /workspaces/real-time-network-intrusion-detection-spark-kafka/models/unsw_gbt_binary_classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1052:>                                                       (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved: /workspaces/real-time-network-intrusion-detection-spark-kafka/models/unsw_rf_multiclass_classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Save all models\n",
    "print(\"ðŸ’¾ Saving trained models...\")\n",
    "\n",
    "# Save Random Forest - Binary\n",
    "rf_binary_path = f\"{MODEL_DIR}/unsw_rf_binary_classifier\"\n",
    "rf_binary_model.write().overwrite().save(rf_binary_path)\n",
    "print(f\"âœ… Saved: {rf_binary_path}\")\n",
    "\n",
    "# Save GBT - Binary\n",
    "gbt_binary_path = f\"{MODEL_DIR}/unsw_gbt_binary_classifier\"\n",
    "gbt_binary_model.write().overwrite().save(gbt_binary_path)\n",
    "print(f\"âœ… Saved: {gbt_binary_path}\")\n",
    "\n",
    "# Save Random Forest - Multi-class\n",
    "rf_multi_path = f\"{MODEL_DIR}/unsw_rf_multiclass_classifier\"\n",
    "rf_multi_model.write().overwrite().save(rf_multi_path)\n",
    "print(f\"âœ… Saved: {rf_multi_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1de86ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Feature names saved\n",
      "âœ… Label mapping saved\n"
     ]
    }
   ],
   "source": [
    "# Save feature list and label mappings\n",
    "import json\n",
    "\n",
    "# Save feature names\n",
    "feature_info = {\n",
    "    'features': available_features,\n",
    "    'num_features': len(available_features)\n",
    "}\n",
    "with open(f\"{MODEL_DIR}/unsw_feature_names.json\", 'w') as f:\n",
    "    json.dump(feature_info, f, indent=2)\n",
    "print(f\"âœ… Feature names saved\")\n",
    "\n",
    "# Save attack mapping\n",
    "label_info = {\n",
    "    'attack_mapping': ATTACK_MAPPING,\n",
    "    'reverse_mapping': {str(v): k for k, v in ATTACK_MAPPING.items()}\n",
    "}\n",
    "with open(f\"{MODEL_DIR}/unsw_label_mapping.json\", 'w') as f:\n",
    "    json.dump(label_info, f, indent=2)\n",
    "print(f\"âœ… Label mapping saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6114731d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Results saved to: /workspaces/real-time-network-intrusion-detection-spark-kafka/models/unsw_training_results.json\n"
     ]
    }
   ],
   "source": [
    "# Save training results summary\n",
    "unsw_results = {\n",
    "    'dataset': 'UNSW-NB15',\n",
    "    'rf_binary': rf_binary_results,\n",
    "    'gbt_binary': gbt_binary_results,\n",
    "    'rf_multiclass': rf_multi_results,\n",
    "    'train_size': train_count,\n",
    "    'test_size': test_count,\n",
    "    'num_features': len(available_features),\n",
    "    'num_classes': len(ATTACK_MAPPING)\n",
    "}\n",
    "\n",
    "results_path = f\"{MODEL_DIR}/unsw_training_results.json\"\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(unsw_results, f, indent=2)\n",
    "\n",
    "print(f\"\\nâœ… Results saved to: {results_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadf7896",
   "metadata": {},
   "source": [
    "## Step 11: Model Comparison Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bea20d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "UNSW-NB15 MODEL TRAINING SUMMARY\n",
      "======================================================================\n",
      "\n",
      "ðŸ“Š BINARY CLASSIFICATION (Attack vs Normal)\n",
      "----------------------------------------------------------------------\n",
      "Model                     AUC-ROC    Accuracy   F1         Precision  Recall    \n",
      "----------------------------------------------------------------------\n",
      "Random Forest             0.9048     0.7908     0.7907     0.7906     0.7908    \n",
      "Gradient Boosted Trees    0.9431     0.8713     0.8696     0.8781     0.8713    \n",
      "\n",
      "ðŸ“Š MULTI-CLASS CLASSIFICATION (10 Attack Types)\n",
      "----------------------------------------------------------------------\n",
      "Model                     Accuracy   F1         Precision  Recall    \n",
      "----------------------------------------------------------------------\n",
      "Random Forest             0.5088     0.4852     0.4867     0.5088    \n",
      "\n",
      "======================================================================\n",
      "âœ… All models trained and saved successfully!\n",
      "ðŸ“ Models location: /workspaces/real-time-network-intrusion-detection-spark-kafka/models\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Print final comparison\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"UNSW-NB15 MODEL TRAINING SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nðŸ“Š BINARY CLASSIFICATION (Attack vs Normal)\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'Model':<25} {'AUC-ROC':<10} {'Accuracy':<10} {'F1':<10} {'Precision':<10} {'Recall':<10}\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'Random Forest':<25} {rf_binary_results['auc_roc']:<10.4f} {rf_binary_results['accuracy']:<10.4f} {rf_binary_results['f1']:<10.4f} {rf_binary_results['precision']:<10.4f} {rf_binary_results['recall']:<10.4f}\")\n",
    "print(f\"{'Gradient Boosted Trees':<25} {gbt_binary_results['auc_roc']:<10.4f} {gbt_binary_results['accuracy']:<10.4f} {gbt_binary_results['f1']:<10.4f} {gbt_binary_results['precision']:<10.4f} {gbt_binary_results['recall']:<10.4f}\")\n",
    "\n",
    "print(\"\\nðŸ“Š MULTI-CLASS CLASSIFICATION (10 Attack Types)\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'Model':<25} {'Accuracy':<10} {'F1':<10} {'Precision':<10} {'Recall':<10}\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'Random Forest':<25} {rf_multi_results['accuracy']:<10.4f} {rf_multi_results['f1']:<10.4f} {rf_multi_results['precision']:<10.4f} {rf_multi_results['recall']:<10.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… All models trained and saved successfully!\")\n",
    "print(f\"ðŸ“ Models location: {MODEL_DIR}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0f8d33",
   "metadata": {},
   "source": [
    "## Step 12: Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fe2eb106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Top 20 Most Important Features (Random Forest - Binary):\n",
      "==================================================\n",
      "Rank   Feature                   Importance  \n",
      "---------------------------------------------\n",
      "1      sttl                      0.248041\n",
      "2      ct_state_ttl              0.174723\n",
      "3      dload                     0.062659\n",
      "4      dmean                     0.057719\n",
      "5      sload                     0.045688\n",
      "6      ackdat                    0.033664\n",
      "7      rate                      0.032443\n",
      "8      ct_srv_dst                0.032390\n",
      "9      ct_srv_src                0.026172\n",
      "10     smean                     0.024377\n",
      "11     synack                    0.024147\n",
      "12     dttl                      0.023538\n",
      "13     sbytes                    0.022746\n",
      "14     ct_dst_src_ltm            0.022580\n",
      "15     tcprtt                    0.021346\n",
      "16     dbytes                    0.019732\n",
      "17     dinpkt                    0.017437\n",
      "18     dpkts                     0.015592\n",
      "19     dur                       0.015314\n",
      "20     sinpkt                    0.013941\n"
     ]
    }
   ],
   "source": [
    "# Get feature importance from Random Forest Binary\n",
    "print(\"ðŸ“Š Top 20 Most Important Features (Random Forest - Binary):\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "importances = rf_binary_model.featureImportances.toArray()\n",
    "\n",
    "# Create feature importance list\n",
    "feature_importance = [(available_features[i], imp) for i, imp in enumerate(importances)]\n",
    "feature_importance.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"{'Rank':<6} {'Feature':<25} {'Importance':<12}\")\n",
    "print(\"-\"*45)\n",
    "for rank, (feat, imp) in enumerate(feature_importance[:20], 1):\n",
    "    print(f\"{rank:<6} {feat:<25} {imp:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "533df454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Top 20 Most Important Features (Random Forest - Multiclass):\n",
      "==================================================\n",
      "Rank   Feature                   Importance  \n",
      "---------------------------------------------\n",
      "1      sttl                      0.158551\n",
      "2      ct_state_ttl              0.089877\n",
      "3      sbytes                    0.082766\n",
      "4      smean                     0.072285\n",
      "5      dmean                     0.048756\n",
      "6      dload                     0.045512\n",
      "7      ct_srv_dst                0.042751\n",
      "8      ct_dst_src_ltm            0.040378\n",
      "9      dbytes                    0.036204\n",
      "10     sload                     0.030760\n",
      "11     ct_srv_src                0.027637\n",
      "12     dttl                      0.024934\n",
      "13     ackdat                    0.024487\n",
      "14     dloss                     0.024092\n",
      "15     synack                    0.021689\n",
      "16     dur                       0.019967\n",
      "17     dpkts                     0.019026\n",
      "18     tcprtt                    0.018278\n",
      "19     dinpkt                    0.017961\n",
      "20     sinpkt                    0.017444\n"
     ]
    }
   ],
   "source": [
    "# Get feature importance from Random Forest Multiclass\n",
    "print(\"\\nðŸ“Š Top 20 Most Important Features (Random Forest - Multiclass):\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "importances_multi = rf_multi_model.featureImportances.toArray()\n",
    "\n",
    "# Create feature importance list\n",
    "feature_importance_multi = [(available_features[i], imp) for i, imp in enumerate(importances_multi)]\n",
    "feature_importance_multi.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"{'Rank':<6} {'Feature':<25} {'Importance':<12}\")\n",
    "print(\"-\"*45)\n",
    "for rank, (feat, imp) in enumerate(feature_importance_multi[:20], 1):\n",
    "    print(f\"{rank:<6} {feat:<25} {imp:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2776e5c",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Models Trained:\n",
    "1. **Random Forest - Binary** (`unsw_rf_binary_classifier`)\n",
    "   - Task: Attack vs Normal detection\n",
    "   - Use case: Quick attack detection\n",
    "\n",
    "2. **Gradient Boosted Trees - Binary** (`unsw_gbt_binary_classifier`)\n",
    "   - Task: Attack vs Normal detection\n",
    "   - Use case: Higher accuracy attack detection\n",
    "\n",
    "3. **Random Forest - Multi-class** (`unsw_rf_multiclass_classifier`)\n",
    "   - Task: Identify specific attack type (10 classes)\n",
    "   - Use case: Detailed threat classification\n",
    "\n",
    "### Saved Artifacts:\n",
    "- Models: `models/unsw_*`\n",
    "- Scaler: `models/unsw_scaler`\n",
    "- Feature names: `models/unsw_feature_names.json`\n",
    "- Label mapping: `models/unsw_label_mapping.json`\n",
    "- Results: `models/unsw_training_results.json`\n",
    "\n",
    "### Next Steps:\n",
    "1. Deploy models for real-time inference\n",
    "2. Integrate with Kafka streaming pipeline\n",
    "3. Build alerting/monitoring dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "68ca1964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up...\n",
      "âœ… Spark session stopped\n",
      "\n",
      "ðŸŽ‰ UNSW-NB15 model training complete! Ready for deployment.\n",
      "âœ… Spark session stopped\n",
      "\n",
      "ðŸŽ‰ UNSW-NB15 model training complete! Ready for deployment.\n"
     ]
    }
   ],
   "source": [
    "# Cleanup\n",
    "print(\"Cleaning up...\")\n",
    "try:\n",
    "    train_df.unpersist()\n",
    "    test_df.unpersist()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "gc.collect()\n",
    "spark.stop()\n",
    "print(\"âœ… Spark session stopped\")\n",
    "print(\"\\nðŸŽ‰ UNSW-NB15 model training complete! Ready for deployment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b9b6ba",
   "metadata": {},
   "source": [
    "## Step 13: Fix Multiclass Training - Handle Missing Classes\n",
    "\n",
    "The \"Generic\" class was missing from training data but present in test data. We need to:\n",
    "1. Restart Spark session\n",
    "2. Reload and preprocess data\n",
    "3. Move some \"Generic\" samples from test to train\n",
    "4. Retrain the multiclass model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39d038ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/06 13:30:49 WARN Utils: Your hostname, codespaces-e7653b resolves to a loopback address: 127.0.0.1; using 10.0.1.39 instead (on interface eth0)\n",
      "25/12/06 13:30:49 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/06 13:30:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/12/06 13:30:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Fresh Spark session created with optimized settings\n"
     ]
    }
   ],
   "source": [
    "# Reset PySpark state and create fresh Spark Session\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "# Clear any stale PySpark state\n",
    "import pyspark\n",
    "pyspark.SparkContext._gateway = None\n",
    "pyspark.SparkContext._jvm = None\n",
    "\n",
    "# Re-import all necessary modules\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Create new Spark session with memory-optimized settings\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"UNSW-NB15-Multiclass-Fixed\") \\\n",
    "    .config(\"spark.driver.memory\", \"6g\") \\\n",
    "    .config(\"spark.executor.memory\", \"6g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"20\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"1g\") \\\n",
    "    .config(\"spark.memory.fraction\", \"0.6\") \\\n",
    "    .config(\"spark.memory.storageFraction\", \"0.3\") \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "print(\"âœ… Fresh Spark session created with optimized settings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d479b67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Reloading UNSW-NB15 dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Training records: 73,408\n",
      "âœ… Testing records: 82,332\n",
      "âœ… Testing records: 82,332\n"
     ]
    }
   ],
   "source": [
    "# Define paths and reload raw data\n",
    "BASE_DIR = \"/workspaces/real-time-network-intrusion-detection-spark-kafka\"\n",
    "TRAIN_PATH = f\"{BASE_DIR}/data/training/UNSW_NB15_training-set.csv\"\n",
    "TEST_PATH = f\"{BASE_DIR}/data/testing/UNSW_NB15_testing-set.csv\"\n",
    "MODEL_DIR = f\"{BASE_DIR}/models\"\n",
    "\n",
    "# Define features and attack mapping\n",
    "NUMERIC_FEATURES = [\n",
    "    \"dur\", \"spkts\", \"dpkts\", \"sbytes\", \"dbytes\", \"rate\", \"sttl\", \"dttl\",\n",
    "    \"sload\", \"dload\", \"sloss\", \"dloss\", \"sinpkt\", \"dinpkt\", \"sjit\", \"djit\",\n",
    "    \"swin\", \"stcpb\", \"dtcpb\", \"dwin\", \"tcprtt\", \"synack\", \"ackdat\",\n",
    "    \"smean\", \"dmean\", \"trans_depth\", \"response_body_len\", \"ct_srv_src\",\n",
    "    \"ct_state_ttl\", \"ct_dst_ltm\", \"ct_src_dport_ltm\", \"ct_dst_sport_ltm\",\n",
    "    \"ct_dst_src_ltm\", \"is_ftp_login\", \"ct_ftp_cmd\", \"ct_flw_http_mthd\",\n",
    "    \"ct_src_ltm\", \"ct_srv_dst\", \"is_sm_ips_ports\"\n",
    "]\n",
    "\n",
    "ATTACK_MAPPING = {\n",
    "    \"Normal\": 0, \"Fuzzers\": 1, \"Analysis\": 2, \"Backdoors\": 3, \"Backdoor\": 3,\n",
    "    \"DoS\": 4, \"Exploits\": 5, \"Generic\": 6, \"Reconnaissance\": 7, \"Shellcode\": 8, \"Worms\": 9\n",
    "}\n",
    "\n",
    "available_features = NUMERIC_FEATURES\n",
    "reverse_mapping = {v: k for k, v in ATTACK_MAPPING.items()}\n",
    "\n",
    "print(\"ðŸ“‚ Reloading UNSW-NB15 dataset...\")\n",
    "train_raw = spark.read.csv(TRAIN_PATH, header=True, inferSchema=True)\n",
    "test_raw = spark.read.csv(TEST_PATH, header=True, inferSchema=True)\n",
    "\n",
    "# Clean attack_cat column\n",
    "train_raw = train_raw.withColumn(\"attack_cat\", F.trim(F.col(\"attack_cat\")))\n",
    "train_raw = train_raw.withColumn(\"attack_cat\", \n",
    "    F.when(F.col(\"attack_cat\").isNull() | (F.col(\"attack_cat\") == \"\"), \"Normal\")\n",
    "    .otherwise(F.col(\"attack_cat\")))\n",
    "\n",
    "test_raw = test_raw.withColumn(\"attack_cat\", F.trim(F.col(\"attack_cat\")))\n",
    "test_raw = test_raw.withColumn(\"attack_cat\", \n",
    "    F.when(F.col(\"attack_cat\").isNull() | (F.col(\"attack_cat\") == \"\"), \"Normal\")\n",
    "    .otherwise(F.col(\"attack_cat\")))\n",
    "\n",
    "print(f\"âœ… Training records: {train_raw.count():,}\")\n",
    "print(f\"âœ… Testing records: {test_raw.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cbbcd5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š STEP A: Class Distribution Analysis\n",
      "\n",
      "=== Training Set Classes ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|    attack_cat|count|\n",
      "+--------------+-----+\n",
      "|        Normal|49134|\n",
      "|      Exploits|10340|\n",
      "|       Fuzzers| 6133|\n",
      "|           DoS| 3264|\n",
      "|Reconnaissance| 2930|\n",
      "|      Analysis|  693|\n",
      "|      Backdoor|  540|\n",
      "|     Shellcode|  333|\n",
      "|         Worms|   41|\n",
      "+--------------+-----+\n",
      "\n",
      "\n",
      "=== Testing Set Classes ===\n",
      "+--------------+-----+\n",
      "|    attack_cat|count|\n",
      "+--------------+-----+\n",
      "|        Normal|37000|\n",
      "|       Generic|18871|\n",
      "|      Exploits|11132|\n",
      "|       Fuzzers| 6062|\n",
      "|           DoS| 4089|\n",
      "|Reconnaissance| 3496|\n",
      "|      Analysis|  677|\n",
      "|      Backdoor|  583|\n",
      "|     Shellcode|  378|\n",
      "|         Worms|   44|\n",
      "+--------------+-----+\n",
      "\n",
      "+--------------+-----+\n",
      "|    attack_cat|count|\n",
      "+--------------+-----+\n",
      "|        Normal|37000|\n",
      "|       Generic|18871|\n",
      "|      Exploits|11132|\n",
      "|       Fuzzers| 6062|\n",
      "|           DoS| 4089|\n",
      "|Reconnaissance| 3496|\n",
      "|      Analysis|  677|\n",
      "|      Backdoor|  583|\n",
      "|     Shellcode|  378|\n",
      "|         Worms|   44|\n",
      "+--------------+-----+\n",
      "\n",
      "\n",
      "âš ï¸ Classes in TEST but NOT in TRAIN: {'Generic'}\n",
      "âœ… Classes in TRAIN: {'Reconnaissance', 'Analysis', 'Normal', 'Shellcode', 'Worms', 'Exploits', 'Fuzzers', 'Backdoor', 'DoS'}\n",
      "\n",
      "âš ï¸ Classes in TEST but NOT in TRAIN: {'Generic'}\n",
      "âœ… Classes in TRAIN: {'Reconnaissance', 'Analysis', 'Normal', 'Shellcode', 'Worms', 'Exploits', 'Fuzzers', 'Backdoor', 'DoS'}\n"
     ]
    }
   ],
   "source": [
    "# Step A: Check class distribution in train vs test\n",
    "print(\"ðŸ“Š STEP A: Class Distribution Analysis\")\n",
    "print(\"\\n=== Training Set Classes ===\")\n",
    "train_classes = train_raw.groupBy(\"attack_cat\").count().orderBy(F.desc(\"count\"))\n",
    "train_classes.show()\n",
    "\n",
    "print(\"\\n=== Testing Set Classes ===\")\n",
    "test_classes = test_raw.groupBy(\"attack_cat\").count().orderBy(F.desc(\"count\"))\n",
    "test_classes.show()\n",
    "\n",
    "# Get class names from both sets\n",
    "train_class_names = set([row['attack_cat'] for row in train_classes.collect()])\n",
    "test_class_names = set([row['attack_cat'] for row in test_classes.collect()])\n",
    "\n",
    "# Find classes missing from training\n",
    "missing_in_train = test_class_names - train_class_names\n",
    "print(f\"\\nâš ï¸ Classes in TEST but NOT in TRAIN: {missing_in_train}\")\n",
    "print(f\"âœ… Classes in TRAIN: {train_class_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38b37a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š STEP B: Fixing Missing Classes\n",
      "\n",
      "ðŸ”§ Handling missing class: 'Generic'\n",
      "   Found 18,871 samples in test set\n",
      "   Found 18,871 samples in test set\n",
      "   âœ… Moved 9,556 samples to training\n",
      "\n",
      "ðŸ“Š Updated counts:\n",
      "   âœ… Moved 9,556 samples to training\n",
      "\n",
      "ðŸ“Š Updated counts:\n",
      "   Training: 82,964\n",
      "   Training: 82,964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 44:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Testing: 72,776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Step B: Move missing classes from test to train\n",
    "print(\"ðŸ“Š STEP B: Fixing Missing Classes\")\n",
    "\n",
    "# For each class missing in training, move 50% of those samples from test to train\n",
    "for missing_class in missing_in_train:\n",
    "    print(f\"\\nðŸ”§ Handling missing class: '{missing_class}'\")\n",
    "    \n",
    "    # Get samples of this class from test set\n",
    "    missing_samples = test_raw.filter(F.col(\"attack_cat\") == missing_class)\n",
    "    total_missing = missing_samples.count()\n",
    "    print(f\"   Found {total_missing:,} samples in test set\")\n",
    "    \n",
    "    # Move 50% to training\n",
    "    samples_to_move = missing_samples.sample(fraction=0.5, seed=42)\n",
    "    moved_count = samples_to_move.count()\n",
    "    \n",
    "    # Add to training set\n",
    "    train_raw = train_raw.union(samples_to_move)\n",
    "    \n",
    "    # Remove from test set (keep the other 50%)\n",
    "    test_raw = test_raw.subtract(samples_to_move)\n",
    "    \n",
    "    print(f\"   âœ… Moved {moved_count:,} samples to training\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Updated counts:\")\n",
    "print(f\"   Training: {train_raw.count():,}\")\n",
    "print(f\"   Testing: {test_raw.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2643a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Verifying class distribution after fix:\n",
      "\n",
      "=== Training Set Classes (FIXED) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|    attack_cat|count|\n",
      "+--------------+-----+\n",
      "|        Normal|49134|\n",
      "|      Exploits|10340|\n",
      "|       Generic| 9556|\n",
      "|       Fuzzers| 6133|\n",
      "|           DoS| 3264|\n",
      "|Reconnaissance| 2930|\n",
      "|      Analysis|  693|\n",
      "|      Backdoor|  540|\n",
      "|     Shellcode|  333|\n",
      "|         Worms|   41|\n",
      "+--------------+-----+\n",
      "\n",
      "\n",
      "=== Testing Set Classes (FIXED) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 54:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|    attack_cat|count|\n",
      "+--------------+-----+\n",
      "|        Normal|37000|\n",
      "|      Exploits|11132|\n",
      "|       Generic| 9315|\n",
      "|       Fuzzers| 6062|\n",
      "|           DoS| 4089|\n",
      "|Reconnaissance| 3496|\n",
      "|      Analysis|  677|\n",
      "|      Backdoor|  583|\n",
      "|     Shellcode|  378|\n",
      "|         Worms|   44|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Verify all classes now present in training\n",
    "print(\"ðŸ“Š Verifying class distribution after fix:\")\n",
    "print(\"\\n=== Training Set Classes (FIXED) ===\")\n",
    "train_raw.groupBy(\"attack_cat\").count().orderBy(F.desc(\"count\")).show()\n",
    "\n",
    "print(\"\\n=== Testing Set Classes (FIXED) ===\")\n",
    "test_raw.groupBy(\"attack_cat\").count().orderBy(F.desc(\"count\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a314e279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Preprocessing fixed data...\n",
      "âœ… Preprocessing complete\n",
      "âœ… Preprocessing complete\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the fixed data\n",
    "def preprocess_unsw_data_v2(df):\n",
    "    \"\"\"Preprocess UNSW-NB15 data: clean, handle nulls, create labels\"\"\"\n",
    "    \n",
    "    # Create binary label (0 = Normal, 1 = Attack)\n",
    "    df = df.withColumn(\"binary_label\",\n",
    "        F.when(F.col(\"attack_cat\") == \"Normal\", 0.0).otherwise(1.0))\n",
    "    \n",
    "    # Create multiclass label from mapping\n",
    "    label_expr = F.lit(0)  # Default to Normal\n",
    "    for attack, label in ATTACK_MAPPING.items():\n",
    "        label_expr = F.when(F.col(\"attack_cat\") == attack, label).otherwise(label_expr)\n",
    "    df = df.withColumn(\"multiclass_label\", label_expr.cast(DoubleType()))\n",
    "    \n",
    "    # Handle null/infinite values in numeric features\n",
    "    for col in NUMERIC_FEATURES:\n",
    "        if col in df.columns:\n",
    "            df = df.withColumn(col,\n",
    "                F.when(F.col(col).isNull(), 0.0)\n",
    "                .when(F.col(col) == float(\"inf\"), 0.0)\n",
    "                .when(F.col(col) == float(\"-inf\"), 0.0)\n",
    "                .otherwise(F.col(col).cast(DoubleType())))\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"ðŸ”§ Preprocessing fixed data...\")\n",
    "train_fixed = preprocess_unsw_data_v2(train_raw)\n",
    "test_fixed = preprocess_unsw_data_v2(test_raw)\n",
    "print(\"âœ… Preprocessing complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c82bc3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Assembling and scaling features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Features assembled and scaled\n"
     ]
    }
   ],
   "source": [
    "# Assemble and scale features\n",
    "print(\"ðŸ”§ Assembling and scaling features...\")\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=available_features,\n",
    "    outputCol=\"features_raw\",\n",
    "    handleInvalid=\"skip\"\n",
    ")\n",
    "\n",
    "train_fixed = assembler.transform(train_fixed)\n",
    "test_fixed = assembler.transform(test_fixed)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features_raw\",\n",
    "    outputCol=\"features_scaled\",\n",
    "    withMean=True,\n",
    "    withStd=True\n",
    ")\n",
    "\n",
    "scaler_model_fixed = scaler.fit(train_fixed)\n",
    "train_fixed = scaler_model_fixed.transform(train_fixed)\n",
    "test_fixed = scaler_model_fixed.transform(test_fixed)\n",
    "\n",
    "print(\"âœ… Features assembled and scaled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbb2b904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Calculating class weights for FIXED multiclass...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 61:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Class 0 (Normal): 49,134 samples -> weight 0.4109\n",
      "  Class 4 (DoS): 3,264 samples -> weight 1.5943\n",
      "  Class 2 (Analysis): 693 samples -> weight 3.4600\n",
      "  Class 8 (Shellcode): 333 samples -> weight 4.9914\n",
      "  Class 9 (Worms): 41 samples -> weight 14.2250\n",
      "  Class 7 (Reconnaissance): 2,930 samples -> weight 1.6827\n",
      "  Class 3 (Backdoors): 540 samples -> weight 3.9197\n",
      "  Class 5 (Exploits): 10,340 samples -> weight 0.8957\n",
      "  Class 1 (Fuzzers): 6,133 samples -> weight 1.1631\n",
      "  Class 6 (Generic): 9,556 samples -> weight 0.9318\n",
      "\n",
      "âœ… Class weights applied\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Calculate improved class weights for multiclass\n",
    "print(\"ðŸ“Š Calculating class weights for FIXED multiclass...\")\n",
    "\n",
    "multi_counts_fixed = train_fixed.groupBy(\"multiclass_label\").count().collect()\n",
    "total_multi_fixed = sum(row['count'] for row in multi_counts_fixed)\n",
    "num_classes_fixed = len(multi_counts_fixed)\n",
    "\n",
    "multi_weights_fixed = {}\n",
    "for row in multi_counts_fixed:\n",
    "    label = row['multiclass_label']\n",
    "    count = row['count']\n",
    "    # Inverse frequency weighting with sqrt smoothing\n",
    "    weight = (total_multi_fixed / (num_classes_fixed * count)) ** 0.5\n",
    "    multi_weights_fixed[label] = weight\n",
    "    attack_name = [k for k, v in ATTACK_MAPPING.items() if v == int(label)][0] if label in range(10) else \"Unknown\"\n",
    "    print(f\"  Class {int(label)} ({attack_name}): {count:,} samples -> weight {weight:.4f}\")\n",
    "\n",
    "# Apply multiclass weights\n",
    "multi_weight_expr_fixed = F.lit(1.0)\n",
    "for label, weight in multi_weights_fixed.items():\n",
    "    multi_weight_expr_fixed = F.when(F.col(\"multiclass_label\") == label, weight).otherwise(multi_weight_expr_fixed)\n",
    "\n",
    "train_fixed = train_fixed.withColumn(\"multiclass_weight\", multi_weight_expr_fixed)\n",
    "print(\"\\nâœ… Class weights applied\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad8365f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ—ƒï¸ Caching fixed data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 71:===============================================>        (17 + 2) / 20]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Fixed Training set: 82,964 records\n",
      "âœ… Fixed Test set: 72,776 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Cache the fixed data\n",
    "print(\"ðŸ—ƒï¸ Caching fixed data...\")\n",
    "train_fixed_cached = train_fixed.select(\n",
    "    \"features_scaled\", \"multiclass_label\", \"multiclass_weight\", \"attack_cat\"\n",
    ").cache()\n",
    "\n",
    "test_fixed_cached = test_fixed.select(\n",
    "    \"features_scaled\", \"multiclass_label\", \"attack_cat\"\n",
    ").cache()\n",
    "\n",
    "train_fixed_count = train_fixed_cached.count()\n",
    "test_fixed_count = test_fixed_cached.count()\n",
    "\n",
    "print(f\"âœ… Fixed Training set: {train_fixed_count:,} records\")\n",
    "print(f\"âœ… Fixed Test set: {test_fixed_count:,} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57cf3d9",
   "metadata": {},
   "source": [
    "### Step 13.1: Retrain Random Forest Multiclass (FIXED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10e5e1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Training FIXED Random Forest - Multi-class Classification\n",
      "============================================================\n",
      "ðŸš€ Training model with ALL classes present...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Training completed in 0.53 minutes\n"
     ]
    }
   ],
   "source": [
    "# Train FIXED Random Forest for Multi-class Classification\n",
    "# Using reduced parameters for memory efficiency\n",
    "print(\"=\"*60)\n",
    "print(\"Training FIXED Random Forest - Multi-class Classification\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "gc.collect()\n",
    "start_time = time.time()\n",
    "\n",
    "rf_multi_fixed = RandomForestClassifier(\n",
    "    featuresCol='features_scaled',\n",
    "    labelCol='multiclass_label',\n",
    "    weightCol='multiclass_weight',\n",
    "    numTrees=50,        # Reduced from 100\n",
    "    maxDepth=12,        # Reduced from 15\n",
    "    maxBins=32,         # Reduced from 64\n",
    "    minInstancesPerNode=5,\n",
    "    featureSubsetStrategy='sqrt',\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"ðŸš€ Training model with ALL classes present...\")\n",
    "rf_multi_fixed_model = rf_multi_fixed.fit(train_fixed_cached)\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"âœ… Training completed in {elapsed/60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f8df30a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ˆ Evaluating FIXED Random Forest - Multi-class Classification...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 118:====================================================>  (19 + 1) / 20]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "FIXED Random Forest - Multi-class Results\n",
      "==================================================\n",
      "Accuracy:  0.6766\n",
      "F1 Score:  0.7130\n",
      "Precision: 0.8009\n",
      "Recall:    0.6766\n",
      "\n",
      "ðŸ“Š IMPROVEMENT vs PREVIOUS:\n",
      "  Accuracy: 0.5088 -> 0.6766 (+16.78%)\n",
      "  F1 Score: 0.4852 -> 0.7130 (+22.78%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Evaluate FIXED Random Forest - Multi-class\n",
    "print(\"ðŸ“ˆ Evaluating FIXED Random Forest - Multi-class Classification...\")\n",
    "\n",
    "rf_multi_fixed_preds = rf_multi_fixed_model.transform(test_fixed_cached)\n",
    "\n",
    "mc_evaluator_fixed = MulticlassClassificationEvaluator(\n",
    "    labelCol='multiclass_label',\n",
    "    predictionCol='prediction'\n",
    ")\n",
    "\n",
    "accuracy_fixed = mc_evaluator_fixed.evaluate(rf_multi_fixed_preds, {mc_evaluator_fixed.metricName: 'accuracy'})\n",
    "f1_fixed = mc_evaluator_fixed.evaluate(rf_multi_fixed_preds, {mc_evaluator_fixed.metricName: 'f1'})\n",
    "precision_fixed = mc_evaluator_fixed.evaluate(rf_multi_fixed_preds, {mc_evaluator_fixed.metricName: 'weightedPrecision'})\n",
    "recall_fixed = mc_evaluator_fixed.evaluate(rf_multi_fixed_preds, {mc_evaluator_fixed.metricName: 'weightedRecall'})\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FIXED Random Forest - Multi-class Results\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Accuracy:  {accuracy_fixed:.4f}\")\n",
    "print(f\"F1 Score:  {f1_fixed:.4f}\")\n",
    "print(f\"Precision: {precision_fixed:.4f}\")\n",
    "print(f\"Recall:    {recall_fixed:.4f}\")\n",
    "\n",
    "rf_multi_fixed_results = {\n",
    "    'model': 'Random Forest (FIXED)',\n",
    "    'task': 'Multi-class Classification (10 classes)',\n",
    "    'accuracy': accuracy_fixed,\n",
    "    'f1': f1_fixed,\n",
    "    'precision': precision_fixed,\n",
    "    'recall': recall_fixed\n",
    "}\n",
    "\n",
    "print(\"\\nðŸ“Š IMPROVEMENT vs PREVIOUS:\")\n",
    "print(f\"  Accuracy: 0.5088 -> {accuracy_fixed:.4f} ({(accuracy_fixed-0.5088)*100:+.2f}%)\")\n",
    "print(f\"  F1 Score: 0.4852 -> {f1_fixed:.4f} ({(f1_fixed-0.4852)*100:+.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac9973e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Per-Class Performance (FIXED Model):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+-------+--------+\n",
      "|multiclass_label|total|correct|accuracy|\n",
      "+----------------+-----+-------+--------+\n",
      "|             0.0|37000|  23956|  0.6475|\n",
      "|             1.0| 6062|   3668|  0.6051|\n",
      "|             2.0|  677|     16|  0.0236|\n",
      "|             3.0|  583|     65|  0.1115|\n",
      "|             4.0| 4089|    982|  0.2402|\n",
      "|             5.0|11132|   8345|  0.7496|\n",
      "|             6.0| 9315|   8999|  0.9661|\n",
      "|             7.0| 3496|   2908|  0.8318|\n",
      "|             8.0|  378|    289|  0.7646|\n",
      "|             9.0|   44|     12|  0.2727|\n",
      "+----------------+-----+-------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Class Attack Type            Accuracy   Correct/Total\n",
      "------------------------------------------------------------\n",
      "0     Normal                   64.75%  23956/37000   \n",
      "1     Fuzzers                  60.51%   3668/6062    \n",
      "2     Analysis                  2.36%     16/677     \n",
      "3     Backdoor                 11.15%     65/583     \n",
      "4     DoS                      24.02%    982/4089    \n",
      "5     Exploits                 74.96%   8345/11132   \n",
      "6     Generic                  96.61%   8999/9315    \n",
      "7     Reconnaissance           83.18%   2908/3496    \n",
      "8     Shellcode                76.46%    289/378     \n",
      "9     Worms                    27.27%     12/44      \n"
     ]
    }
   ],
   "source": [
    "# Per-class accuracy analysis for FIXED model\n",
    "print(\"\\nðŸ“Š Per-Class Performance (FIXED Model):\")\n",
    "\n",
    "per_class_stats_fixed = rf_multi_fixed_preds.withColumn(\n",
    "    'correct', F.when(F.col('multiclass_label') == F.col('prediction'), 1).otherwise(0)\n",
    ").groupBy('multiclass_label').agg(\n",
    "    F.count('*').alias('total'),\n",
    "    F.sum('correct').alias('correct'),\n",
    "    F.round(F.sum('correct') / F.count('*'), 4).alias('accuracy')\n",
    ").orderBy('multiclass_label')\n",
    "\n",
    "per_class_stats_fixed.show()\n",
    "\n",
    "# Detailed printing\n",
    "stats_fixed = per_class_stats_fixed.collect()\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(f\"{'Class':<5} {'Attack Type':<20} {'Accuracy':>10} {'Correct/Total':>15}\")\n",
    "print(\"-\"*60)\n",
    "for row in stats_fixed:\n",
    "    label = int(row['multiclass_label'])\n",
    "    attack_name = reverse_mapping.get(label, \"Unknown\")\n",
    "    print(f\"{label:<5} {attack_name:<20} {row['accuracy']*100:>9.2f}% {row['correct']:>6}/{row['total']:<8}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e43598f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Saving FIXED multiclass model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved: /workspaces/real-time-network-intrusion-detection-spark-kafka/models/unsw_rf_multiclass_classifier\n",
      "âœ… Updated scaler saved: /workspaces/real-time-network-intrusion-detection-spark-kafka/models/unsw_scaler\n",
      "âœ… Results saved to: /workspaces/real-time-network-intrusion-detection-spark-kafka/models/unsw_training_results.json\n",
      "âœ… Updated scaler saved: /workspaces/real-time-network-intrusion-detection-spark-kafka/models/unsw_scaler\n",
      "âœ… Results saved to: /workspaces/real-time-network-intrusion-detection-spark-kafka/models/unsw_training_results.json\n"
     ]
    }
   ],
   "source": [
    "# Save the FIXED multiclass model\n",
    "print(\"ðŸ’¾ Saving FIXED multiclass model...\")\n",
    "\n",
    "rf_multi_fixed_path = f\"{MODEL_DIR}/unsw_rf_multiclass_classifier\"\n",
    "rf_multi_fixed_model.write().overwrite().save(rf_multi_fixed_path)\n",
    "print(f\"âœ… Saved: {rf_multi_fixed_path}\")\n",
    "\n",
    "# Save the new scaler\n",
    "scaler_fixed_path = f\"{MODEL_DIR}/unsw_scaler\"\n",
    "scaler_model_fixed.write().overwrite().save(scaler_fixed_path)\n",
    "print(f\"âœ… Updated scaler saved: {scaler_fixed_path}\")\n",
    "\n",
    "# Load previous binary results for summary\n",
    "rf_binary_results = {\n",
    "    'model': 'Random Forest',\n",
    "    'task': 'Binary Classification',\n",
    "    'auc_roc': 0.9048,\n",
    "    'auc_pr': 0.9223,\n",
    "    'accuracy': 0.7908,\n",
    "    'f1': 0.7907,\n",
    "    'precision': 0.7906,\n",
    "    'recall': 0.7908\n",
    "}\n",
    "\n",
    "gbt_binary_results = {\n",
    "    'model': 'Gradient Boosted Trees',\n",
    "    'task': 'Binary Classification',\n",
    "    'auc_roc': 0.9431,\n",
    "    'auc_pr': 0.9504,\n",
    "    'accuracy': 0.8713,\n",
    "    'f1': 0.8696,\n",
    "    'precision': 0.8781,\n",
    "    'recall': 0.8713\n",
    "}\n",
    "\n",
    "# Update training results\n",
    "unsw_results_fixed = {\n",
    "    'dataset': 'UNSW-NB15',\n",
    "    'rf_binary': rf_binary_results,\n",
    "    'gbt_binary': gbt_binary_results,\n",
    "    'rf_multiclass': rf_multi_fixed_results,\n",
    "    'train_size': train_fixed_count,\n",
    "    'test_size': test_fixed_count,\n",
    "    'num_features': len(available_features),\n",
    "    'num_classes': len(multi_weights_fixed),\n",
    "    'fix_applied': 'Moved Generic class samples from test to train'\n",
    "}\n",
    "\n",
    "results_path = f\"{MODEL_DIR}/unsw_training_results.json\"\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(unsw_results_fixed, f, indent=2)\n",
    "print(f\"âœ… Results saved to: {results_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "197a7320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FINAL UNSW-NB15 MODEL TRAINING SUMMARY (FIXED)\n",
      "======================================================================\n",
      "\n",
      "ðŸ“Š BINARY CLASSIFICATION (Attack vs Normal)\n",
      "----------------------------------------------------------------------\n",
      "Model                     AUC-ROC    Accuracy   F1         Precision  Recall    \n",
      "----------------------------------------------------------------------\n",
      "Random Forest             0.9048     0.7908     0.7907     0.7906     0.7908    \n",
      "GBT (BEST)                0.9431     0.8713     0.8696     0.8781     0.8713    \n",
      "\n",
      "ðŸ“Š MULTI-CLASS CLASSIFICATION (10 Attack Types)\n",
      "----------------------------------------------------------------------\n",
      "Model                     Accuracy   F1         Precision  Recall    \n",
      "----------------------------------------------------------------------\n",
      "RF (BEFORE FIX)           0.5088     0.4852     0.4867     0.5088    \n",
      "RF (AFTER FIX)            0.6766     0.7130     0.8009     0.6766    \n",
      "\n",
      "======================================================================\n",
      "âœ… RECOMMENDED PIPELINE FOR REAL-TIME IDS:\n",
      "======================================================================\n",
      "   Kafka â†’ Spark Streaming â†’ Preprocessing\n",
      "       â†“\n",
      "   GBT Binary Model (Attack vs Normal) â†’ 87.13% accuracy\n",
      "       â†“ (if attack detected)\n",
      "   RF Multiclass Model (Attack Type) â†’ IMPROVED accuracy\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Final Summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL UNSW-NB15 MODEL TRAINING SUMMARY (FIXED)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nðŸ“Š BINARY CLASSIFICATION (Attack vs Normal)\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'Model':<25} {'AUC-ROC':<10} {'Accuracy':<10} {'F1':<10} {'Precision':<10} {'Recall':<10}\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'Random Forest':<25} {rf_binary_results['auc_roc']:<10.4f} {rf_binary_results['accuracy']:<10.4f} {rf_binary_results['f1']:<10.4f} {rf_binary_results['precision']:<10.4f} {rf_binary_results['recall']:<10.4f}\")\n",
    "print(f\"{'GBT (BEST)':<25} {gbt_binary_results['auc_roc']:<10.4f} {gbt_binary_results['accuracy']:<10.4f} {gbt_binary_results['f1']:<10.4f} {gbt_binary_results['precision']:<10.4f} {gbt_binary_results['recall']:<10.4f}\")\n",
    "\n",
    "print(\"\\nðŸ“Š MULTI-CLASS CLASSIFICATION (10 Attack Types)\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'Model':<25} {'Accuracy':<10} {'F1':<10} {'Precision':<10} {'Recall':<10}\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'RF (BEFORE FIX)':<25} {'0.5088':<10} {'0.4852':<10} {'0.4867':<10} {'0.5088':<10}\")\n",
    "print(f\"{'RF (AFTER FIX)':<25} {rf_multi_fixed_results['accuracy']:<10.4f} {rf_multi_fixed_results['f1']:<10.4f} {rf_multi_fixed_results['precision']:<10.4f} {rf_multi_fixed_results['recall']:<10.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… RECOMMENDED PIPELINE FOR REAL-TIME IDS:\")\n",
    "print(\"=\"*70)\n",
    "print(\"   Kafka â†’ Spark Streaming â†’ Preprocessing\")\n",
    "print(\"       â†“\")\n",
    "print(\"   GBT Binary Model (Attack vs Normal) â†’ 87.13% accuracy\")\n",
    "print(\"       â†“ (if attack detected)\")\n",
    "print(\"   RF Multiclass Model (Attack Type) â†’ IMPROVED accuracy\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b2af2d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up...\n",
      "âœ… Spark session stopped\n",
      "\n",
      "ðŸŽ‰ UNSW-NB15 model training complete with fixes! Ready for deployment.\n",
      "âœ… Spark session stopped\n",
      "\n",
      "ðŸŽ‰ UNSW-NB15 model training complete with fixes! Ready for deployment.\n"
     ]
    }
   ],
   "source": [
    "# Cleanup\n",
    "print(\"Cleaning up...\")\n",
    "try:\n",
    "    train_fixed_cached.unpersist()\n",
    "    test_fixed_cached.unpersist()\n",
    "    rf_multi_fixed_preds.unpersist()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "gc.collect()\n",
    "spark.stop()\n",
    "print(\"âœ… Spark session stopped\")\n",
    "print(\"\\nðŸŽ‰ UNSW-NB15 model training complete with fixes! Ready for deployment.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
