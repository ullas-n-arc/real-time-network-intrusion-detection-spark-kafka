{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c60a90d7",
   "metadata": {},
   "source": [
    "# Network Intrusion Detection Dataset Preprocessing\n",
    "## Google Colab / VS Code Jupyter Notebook\n",
    "\n",
    "This notebook preprocesses three network intrusion detection datasets:\n",
    "- **CIC-IDS 2017**\n",
    "- **CIC-IDS 2018**\n",
    "- **UNSW-NB15**\n",
    "\n",
    "### Features:\n",
    "- ‚úÖ Class imbalance handling with sample weights\n",
    "- ‚úÖ Complete data cleaning pipeline\n",
    "- ‚úÖ Feature scaling and standardization\n",
    "- ‚úÖ Parquet export for efficient storage\n",
    "- ‚úÖ Google Drive integration\n",
    "\n",
    "### Prerequisites:\n",
    "1. Upload your dataset CSV files to Google Drive\n",
    "2. Run this notebook in Google Colab or VS Code with Jupyter extension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37846248",
   "metadata": {},
   "source": [
    "## Step 1: Install and Setup PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efa3472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PySpark (if not already installed in Colab)\n",
    "!pip install -q pyspark\n",
    "\n",
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, isnan, isnull, count, lit, lower, trim\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import DoubleType, FloatType, IntegerType, LongType, StringType\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer\n",
    "from functools import reduce\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"‚úÖ PySpark installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e845227",
   "metadata": {},
   "source": [
    "## Step 2: Mount Google Drive (for Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff09b8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (only for Colab)\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    # Path to your data in Google Drive\n",
    "    BASE_DIR = \"/content/drive/MyDrive/NetworkIDS\"\n",
    "    print(f\"‚úÖ Google Drive mounted successfully!\")\n",
    "    print(f\"üìÅ Data directory: {BASE_DIR}\")\n",
    "    IS_COLAB = True\n",
    "except:\n",
    "    # Running locally in VS Code\n",
    "    BASE_DIR = \"d:/Coding/real-time-network-intrusion-detection-spark-kafka/data\"\n",
    "    print(f\"‚úÖ Running locally. Data directory: {BASE_DIR}\")\n",
    "    IS_COLAB = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50feb4fd",
   "metadata": {},
   "source": [
    "## Step 3: Create Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d76e180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create optimized Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"NetworkIDS-Preprocessing\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"12g\" if IS_COLAB else \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"12g\" if IS_COLAB else \"4g\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .config(\"spark.driver.host\", \"localhost\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(f\"‚úÖ Spark Session created\")\n",
    "print(f\"üìä Spark UI: {spark.sparkContext.uiWebUrl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd2698e",
   "metadata": {},
   "source": [
    "## Step 4: Preprocessing Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c79180",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_column_names(df):\n",
    "    \"\"\"Clean column names by removing special characters\"\"\"\n",
    "    for column in df.columns:\n",
    "        new_name = ''.join(c if c.isalnum() or c == '_' else '_' for c in column)\n",
    "        new_name = new_name.lower().strip('_')\n",
    "        while '__' in new_name:\n",
    "            new_name = new_name.replace('__', '_')\n",
    "        if new_name != column:\n",
    "            df = df.withColumnRenamed(column, new_name)\n",
    "    return df\n",
    "\n",
    "def handle_missing_values(df, label_col):\n",
    "    \"\"\"Handle missing values by filling with mean - OPTIMIZED\"\"\"\n",
    "    numeric_cols = [f.name for f in df.schema.fields \n",
    "                   if isinstance(f.dataType, (DoubleType, IntegerType, LongType)) \n",
    "                   and f.name != label_col]\n",
    "    \n",
    "    # Calculate all means in ONE PASS\n",
    "    mean_exprs = [F.mean(col_name).alias(col_name) for col_name in numeric_cols]\n",
    "    means_row = df.select(*mean_exprs).first()\n",
    "    means_dict = means_row.asDict()\n",
    "    \n",
    "    # Fill missing values using precomputed means\n",
    "    for col_name in numeric_cols:\n",
    "        mean_val = means_dict[col_name]\n",
    "        if mean_val is not None:\n",
    "            df = df.withColumn(col_name, \n",
    "                             F.when(F.col(col_name).isNull(), F.lit(mean_val))\n",
    "                             .otherwise(F.col(col_name)))\n",
    "    return df\n",
    "\n",
    "def handle_infinite_values(df, label_col):\n",
    "    \"\"\"Replace infinite values with 0 - OPTIMIZED\"\"\"\n",
    "    numeric_cols = [f.name for f in df.schema.fields \n",
    "                   if isinstance(f.dataType, (DoubleType, IntegerType, LongType)) \n",
    "                   and f.name != label_col]\n",
    "    \n",
    "    for col_name in numeric_cols:\n",
    "        df = df.withColumn(col_name, \n",
    "                          F.when(F.col(col_name).isin([float('inf'), float('-inf')]), 0)\n",
    "                          .otherwise(F.col(col_name)))\n",
    "    return df\n",
    "\n",
    "def calculate_class_weights(df, label_col):\n",
    "    \"\"\"Calculate inverse frequency weights for handling class imbalance\"\"\"\n",
    "    class_counts = df.groupBy(label_col).count().collect()\n",
    "    max_count = max([row['count'] for row in class_counts])\n",
    "    weights = {row[label_col]: float(max_count) / row['count'] for row in class_counts}\n",
    "    return weights\n",
    "\n",
    "def add_sample_weights(df, label_col, weights):\n",
    "    \"\"\"Add sample_weight column based on class weights\"\"\"\n",
    "    weight_expr = F.when(F.col(label_col) == list(weights.keys())[0], list(weights.values())[0])\n",
    "    for label_val, weight_val in list(weights.items())[1:]:\n",
    "        weight_expr = weight_expr.when(F.col(label_col) == label_val, weight_val)\n",
    "    df = df.withColumn('sample_weight', weight_expr)\n",
    "    return df\n",
    "\n",
    "def create_binary_label(df, label_col, benign_value):\n",
    "    \"\"\"Create binary label (0=benign, 1=attack)\"\"\"\n",
    "    if isinstance(benign_value, list):\n",
    "        benign_value = benign_value[0]\n",
    "    \n",
    "    df = df.withColumn(\n",
    "        'binary_label',\n",
    "        F.when(F.col(label_col) == benign_value, 0).otherwise(1)\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def scale_features(df, exclude_cols, output_dir, dataset_name):\n",
    "    \"\"\"Scale features using StandardScaler - FIXED for string columns\"\"\"\n",
    "    from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "    from pyspark.sql.types import StringType, TimestampType\n",
    "    \n",
    "    # Automatically determine feature columns (exclude labels, metadata, and non-numeric)\n",
    "    exclude_list = exclude_cols if isinstance(exclude_cols, list) else [exclude_cols]\n",
    "    exclude_list.extend(['binary_label', 'sample_weight'])\n",
    "    \n",
    "    # Only include numeric columns (exclude strings and timestamps)\n",
    "    feature_cols = [f.name for f in df.schema.fields \n",
    "                   if f.name not in exclude_list \n",
    "                   and not isinstance(f.dataType, (StringType, TimestampType))]\n",
    "    \n",
    "    print(f\"  Using {len(feature_cols)} numeric features for scaling\")\n",
    "    \n",
    "    assembler = VectorAssembler(inputCols=feature_cols, outputCol='features_temp', handleInvalid='skip')\n",
    "    df_assembled = assembler.transform(df)\n",
    "    \n",
    "    scaler = StandardScaler(inputCol='features_temp', outputCol='features_scaled', withMean=True, withStd=True)\n",
    "    scaler_model = scaler.fit(df_assembled)\n",
    "    df_scaled = scaler_model.transform(df_assembled)\n",
    "    \n",
    "    scaler_model.write().overwrite().save(f\"{output_dir}/models/{dataset_name}_scaler\")\n",
    "    \n",
    "    return df_scaled.drop('features_temp'), feature_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0a043a",
   "metadata": {},
   "source": [
    "## Step 5: Dataset-Specific Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c84574",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cicids2017(data_dir):\n",
    "    \"\"\"Load CIC-IDS 2017 dataset\"\"\"\n",
    "    files = [\n",
    "        'Monday-WorkingHours.pcap_ISCX.csv',\n",
    "        'Tuesday-WorkingHours.pcap_ISCX.csv',\n",
    "        'Wednesday-workingHours.pcap_ISCX.csv',\n",
    "        'Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv',\n",
    "        'Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv',\n",
    "        'Friday-WorkingHours-Morning.pcap_ISCX.csv',\n",
    "        'Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv',\n",
    "        'Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv'\n",
    "    ]\n",
    "    \n",
    "    print(f\"Loading {len(files)} CIC-IDS 2017 files...\")\n",
    "    dfs = []\n",
    "    for file in files:\n",
    "        path = f\"{data_dir}/CSE-CIC-IDS2017/{file}\"\n",
    "        print(f\"  Loading {file}...\")\n",
    "        df = spark.read.csv(path, header=True, inferSchema=True)\n",
    "        dfs.append(df)\n",
    "    \n",
    "    print(\"Combining all files...\")\n",
    "    return reduce(lambda df1, df2: df1.union(df2), dfs)\n",
    "\n",
    "def load_cicids2018(data_dir):\n",
    "    \"\"\"Load CIC-IDS 2018 dataset\"\"\"\n",
    "    files = [\n",
    "        '02-14-2018.csv', '02-15-2018.csv', '02-16-2018.csv',\n",
    "        '02-20-2018.csv', '02-21-2018.csv', '02-22-2018.csv',\n",
    "        '02-23-2018.csv', '02-28-2018.csv', '03-01-2018.csv', '03-02-2018.csv'\n",
    "    ]\n",
    "    \n",
    "    print(f\"Loading {len(files)} CIC-IDS 2018 files...\")\n",
    "    dfs = []\n",
    "    for file in files:\n",
    "        path = f\"{data_dir}/CSE-CIC-IDS2018/{file}\"\n",
    "        print(f\"  Loading {file}...\")\n",
    "        df = spark.read.csv(path, header=True, inferSchema=True)\n",
    "        \n",
    "        # Drop identifier columns that cause schema mismatch\n",
    "        cols_to_drop = ['Flow ID', 'Src IP', 'Src Port', 'Dst IP']\n",
    "        for col in cols_to_drop:\n",
    "            if col in df.columns:\n",
    "                df = df.drop(col)\n",
    "        \n",
    "        dfs.append(df)\n",
    "    \n",
    "    print(\"Combining all files...\")\n",
    "    return reduce(lambda df1, df2: df1.union(df2), dfs)\n",
    "\n",
    "def load_unsw_nb15(data_dir):\n",
    "    \"\"\"Load UNSW-NB15 dataset\"\"\"\n",
    "    train_path = f\"{data_dir}/UNSW-NB15/UNSW_NB15_training-set.csv\"\n",
    "    test_path = f\"{data_dir}/UNSW-NB15/UNSW_NB15_testing-set.csv\"\n",
    "    \n",
    "    print(\"Loading UNSW-NB15 training set...\")\n",
    "    train_df = spark.read.csv(train_path, header=True, inferSchema=True)\n",
    "    print(\"Loading UNSW-NB15 testing set...\")\n",
    "    test_df = spark.read.csv(test_path, header=True, inferSchema=True)\n",
    "    \n",
    "    print(\"Combining train and test sets...\")\n",
    "    return train_df.union(test_df)\n",
    "\n",
    "def encode_categorical_features(df):\n",
    "    \"\"\"Encode categorical features for UNSW-NB15 - OPTIMIZED\"\"\"\n",
    "    from pyspark.ml.feature import StringIndexer\n",
    "    \n",
    "    categorical_cols = ['proto', 'service', 'state']\n",
    "    \n",
    "    # Use StringIndexer for each categorical column\n",
    "    for col in categorical_cols:\n",
    "        if col in df.columns:\n",
    "            print(f\"  Encoding {col}...\")\n",
    "            indexer = StringIndexer(inputCol=col, outputCol=f\"{col}_index\", handleInvalid=\"keep\")\n",
    "            df = indexer.fit(df).transform(df)\n",
    "            df = df.drop(col)  # Drop original categorical column\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bad2b98",
   "metadata": {},
   "source": [
    "## Step 6: Preprocessing Pipeline Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1ee485",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_cicids2017(df, output_dir):\n",
    "    \"\"\"Full preprocessing pipeline for CIC-IDS 2017\"\"\"\n",
    "    print(\"üîÑ Starting CIC-IDS 2017 preprocessing...\")\n",
    "    \n",
    "    # Step 1: Clean column names\n",
    "    print(\"Step 1: Cleaning column names...\")\n",
    "    df = clean_column_names(df)\n",
    "    \n",
    "    # Step 2: Deduplicate\n",
    "    print(\"Step 2: Removing duplicates...\")\n",
    "    initial_count = df.count()\n",
    "    df = df.dropDuplicates()\n",
    "    final_count = df.count()\n",
    "    print(f\"  Removed {initial_count - final_count:,} duplicates\")\n",
    "    \n",
    "    # Step 3: Handle missing values\n",
    "    print(\"Step 3: Handling missing values...\")\n",
    "    df = handle_missing_values(df, 'label')\n",
    "    \n",
    "    # Step 4: Handle infinite values\n",
    "    print(\"Step 4: Handling infinite values...\")\n",
    "    df = handle_infinite_values(df, 'label')\n",
    "    \n",
    "    # Step 5: Create binary label\n",
    "    print(\"Step 5: Creating binary label...\")\n",
    "    df = create_binary_label(df, 'label', 'BENIGN')\n",
    "    \n",
    "    # Step 6: Calculate and add sample weights\n",
    "    print(\"Step 6: Calculating class weights...\")\n",
    "    class_weights = calculate_class_weights(df, 'binary_label')\n",
    "    df = add_sample_weights(df, 'binary_label', class_weights)\n",
    "    \n",
    "    # Step 7: Scale features\n",
    "    print(\"Step 7: Scaling features...\")\n",
    "    df, feature_cols = scale_features(df, 'label', output_dir, 'cicids2017')\n",
    "    \n",
    "    # Step 8: Save to parquet\n",
    "    print(\"Step 8: Saving to parquet...\")\n",
    "    parquet_path = f\"{output_dir}/parquet/cicids2017_preprocessed\"\n",
    "    df.write.mode('overwrite').parquet(parquet_path)\n",
    "    print(f\"‚úÖ Saved to {parquet_path}\")\n",
    "    \n",
    "    return df, class_weights, feature_cols\n",
    "\n",
    "def preprocess_cicids2018(df, output_dir):\n",
    "    \"\"\"Full preprocessing pipeline for CIC-IDS 2018\"\"\"\n",
    "    print(\"üîÑ Starting CIC-IDS 2018 preprocessing...\")\n",
    "    \n",
    "    print(\"Step 1: Cleaning column names...\")\n",
    "    df = clean_column_names(df)\n",
    "    \n",
    "    print(\"Step 2: Removing duplicates...\")\n",
    "    initial_count = df.count()\n",
    "    df = df.dropDuplicates()\n",
    "    final_count = df.count()\n",
    "    print(f\"  Removed {initial_count - final_count:,} duplicates\")\n",
    "    \n",
    "    print(\"Step 3: Handling missing values...\")\n",
    "    df = handle_missing_values(df, 'label')\n",
    "    \n",
    "    print(\"Step 4: Handling infinite values...\")\n",
    "    df = handle_infinite_values(df, 'label')\n",
    "    \n",
    "    print(\"Step 5: Creating binary label...\")\n",
    "    df = create_binary_label(df, 'label', 'Benign')\n",
    "    \n",
    "    print(\"Step 6: Calculating class weights...\")\n",
    "    class_weights = calculate_class_weights(df, 'binary_label')\n",
    "    df = add_sample_weights(df, 'binary_label', class_weights)\n",
    "    \n",
    "    print(\"Step 7: Scaling features...\")\n",
    "    df, feature_cols = scale_features(df, 'label', output_dir, 'cicids2018')\n",
    "    \n",
    "    print(\"Step 8: Saving to parquet...\")\n",
    "    parquet_path = f\"{output_dir}/parquet/cicids2018_preprocessed\"\n",
    "    df.write.mode('overwrite').parquet(parquet_path)\n",
    "    print(f\"‚úÖ Saved to {parquet_path}\")\n",
    "    \n",
    "    return df, class_weights, feature_cols\n",
    "\n",
    "def preprocess_unsw_nb15(df, output_dir):\n",
    "    \"\"\"Full preprocessing pipeline for UNSW-NB15\"\"\"\n",
    "    print(\"üîÑ Starting UNSW-NB15 preprocessing...\")\n",
    "    \n",
    "    print(\"Step 1: Cleaning column names...\")\n",
    "    df = clean_column_names(df)\n",
    "    \n",
    "    print(\"Step 2: Encoding categorical features...\")\n",
    "    df = encode_categorical_features(df)\n",
    "    \n",
    "    print(\"Step 3: Removing duplicates...\")\n",
    "    initial_count = df.count()\n",
    "    df = df.dropDuplicates()\n",
    "    final_count = df.count()\n",
    "    print(f\"  Removed {initial_count - final_count:,} duplicates\")\n",
    "    \n",
    "    print(\"Step 4: Handling missing values...\")\n",
    "    df = handle_missing_values(df, 'label')\n",
    "    \n",
    "    print(\"Step 5: Handling infinite values...\")\n",
    "    df = handle_infinite_values(df, 'label')\n",
    "    \n",
    "    print(\"Step 6: Renaming label to binary_label...\")\n",
    "    df = df.withColumnRenamed('label', 'binary_label')\n",
    "    \n",
    "    print(\"Step 7: Calculating class weights...\")\n",
    "    class_weights = calculate_class_weights(df, 'binary_label')\n",
    "    df = add_sample_weights(df, 'binary_label', class_weights)\n",
    "    \n",
    "    print(\"Step 8: Scaling features...\")\n",
    "    df, feature_cols = scale_features(df, 'attack_cat', output_dir, 'unsw_nb15')\n",
    "    \n",
    "    print(\"Step 9: Saving to parquet...\")\n",
    "    parquet_path = f\"{output_dir}/parquet/unsw_nb15_preprocessed\"\n",
    "    df.write.mode('overwrite').parquet(parquet_path)\n",
    "    print(f\"‚úÖ Saved to {parquet_path}\")\n",
    "    \n",
    "    return df, class_weights, feature_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f1dd40",
   "metadata": {},
   "source": [
    "## Step 7: Run Preprocessing for All Datasets\n",
    "\n",
    "**‚ö†Ô∏è IMPORTANT**: Before running this cell, ensure your datasets are uploaded to Google Drive at:\n",
    "- `/content/drive/MyDrive/NetworkIDS/CSE-CIC-IDS2017/`\n",
    "- `/content/drive/MyDrive/NetworkIDS/CSE-CIC-IDS2018/`\n",
    "- `/content/drive/MyDrive/NetworkIDS/UNSW-NB15/`\n",
    "\n",
    "This will take **30-60 minutes** depending on dataset size and Colab resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3aa1c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which datasets are already processed (checkpoint recovery)\n",
    "import os\n",
    "processed = []\n",
    "if os.path.exists(f\"{BASE_DIR}/output/parquet/cicids2017_preprocessed\"):\n",
    "    processed.append(\"cicids2017\")\n",
    "    print(\"‚úÖ CIC-IDS 2017 already processed - will skip\")\n",
    "if os.path.exists(f\"{BASE_DIR}/output/parquet/cicids2018_preprocessed\"):\n",
    "    processed.append(\"cicids2018\")\n",
    "    print(\"‚úÖ CIC-IDS 2018 already processed - will skip\")\n",
    "if os.path.exists(f\"{BASE_DIR}/output/parquet/unsw_nb15_preprocessed\"):\n",
    "    processed.append(\"unsw_nb15\")\n",
    "    print(\"‚úÖ UNSW-NB15 already processed - will skip\")\n",
    "\n",
    "if not processed:\n",
    "    print(\"üìù No datasets processed yet. Starting fresh...\")\n",
    "else:\n",
    "    print(f\"üìù Found {len(processed)} already processed dataset(s). Resuming...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb360f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import time\n",
    "\n",
    "# Re-mount Google Drive if connection lost (important for long sessions)\n",
    "if IS_COLAB:\n",
    "    try:\n",
    "        # Test if drive is still mounted\n",
    "        os.listdir(BASE_DIR)\n",
    "        print(\"‚úÖ Google Drive connection verified\")\n",
    "    except:\n",
    "        print(\"‚ö†Ô∏è Drive disconnected! Remounting...\")\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive', force_remount=True)\n",
    "        print(\"‚úÖ Drive remounted successfully\")\n",
    "\n",
    "# Set paths\n",
    "DATA_DIR = BASE_DIR  # Uses mounted drive or local data/\n",
    "OUTPUT_DIR = f\"{BASE_DIR}/output\"\n",
    "\n",
    "# Create output directories\n",
    "import os\n",
    "os.makedirs(f\"{OUTPUT_DIR}/models\", exist_ok=True)\n",
    "os.makedirs(f\"{OUTPUT_DIR}/parquet\", exist_ok=True)\n",
    "\n",
    "results = {}\n",
    "\n",
    "# ===== CIC-IDS 2017 =====\n",
    "if 'cicids2017' not in processed:\n",
    "    try:\n",
    "        start = time.time()\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üì¶ PROCESSING CIC-IDS 2017\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        df_2017 = load_cicids2017(DATA_DIR)\n",
    "        df_2017, weights_2017, features_2017 = preprocess_cicids2017(df_2017, OUTPUT_DIR)\n",
    "        \n",
    "        elapsed = time.time() - start\n",
    "        results['cicids2017'] = {\n",
    "            'status': 'SUCCESS',\n",
    "            'time': f\"{elapsed/60:.2f} minutes\",\n",
    "            'class_weights': weights_2017,\n",
    "            'num_features': len(features_2017)\n",
    "        }\n",
    "        print(f\"‚úÖ CIC-IDS 2017 completed in {elapsed/60:.2f} minutes\")\n",
    "        print(f\"üíæ Saved to: {OUTPUT_DIR}/parquet/cicids2017_preprocessed\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        results['cicids2017'] = {'status': 'FAILED', 'error': str(e)}\n",
    "        print(f\"‚ùå CIC-IDS 2017 failed: {e}\")\n",
    "else:\n",
    "    results['cicids2017'] = {'status': 'SKIPPED', 'reason': 'Already processed'}\n",
    "    print(\"‚è≠Ô∏è Skipping CIC-IDS 2017 (already processed)\")\n",
    "\n",
    "# ===== CIC-IDS 2018 =====\n",
    "if 'cicids2018' not in processed:\n",
    "    try:\n",
    "        start = time.time()\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üì¶ PROCESSING CIC-IDS 2018\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        df_2018 = load_cicids2018(DATA_DIR)\n",
    "        df_2018, weights_2018, features_2018 = preprocess_cicids2018(df_2018, OUTPUT_DIR)\n",
    "        \n",
    "        elapsed = time.time() - start\n",
    "        results['cicids2018'] = {\n",
    "            'status': 'SUCCESS',\n",
    "            'time': f\"{elapsed/60:.2f} minutes\",\n",
    "            'class_weights': weights_2018,\n",
    "            'num_features': len(features_2018)\n",
    "        }\n",
    "        print(f\"‚úÖ CIC-IDS 2018 completed in {elapsed/60:.2f} minutes\")\n",
    "        print(f\"üíæ Saved to: {OUTPUT_DIR}/parquet/cicids2018_preprocessed\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        results['cicids2018'] = {'status': 'FAILED', 'error': str(e)}\n",
    "        print(f\"‚ùå CIC-IDS 2018 failed: {e}\")\n",
    "else:\n",
    "    results['cicids2018'] = {'status': 'SKIPPED', 'reason': 'Already processed'}\n",
    "    print(\"‚è≠Ô∏è Skipping CIC-IDS 2018 (already processed)\")\n",
    "\n",
    "# ===== UNSW-NB15 =====\n",
    "if 'unsw_nb15' not in processed:\n",
    "    try:\n",
    "        start = time.time()\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üì¶ PROCESSING UNSW-NB15\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        df_unsw = load_unsw_nb15(DATA_DIR)\n",
    "        df_unsw, weights_unsw, features_unsw = preprocess_unsw_nb15(df_unsw, OUTPUT_DIR)\n",
    "        \n",
    "        elapsed = time.time() - start\n",
    "        results['unsw_nb15'] = {\n",
    "            'status': 'SUCCESS',\n",
    "            'time': f\"{elapsed/60:.2f} minutes\",\n",
    "            'class_weights': weights_unsw,\n",
    "            'num_features': len(features_unsw)\n",
    "        }\n",
    "        print(f\"‚úÖ UNSW-NB15 completed in {elapsed/60:.2f} minutes\")\n",
    "        print(f\"üíæ Saved to: {OUTPUT_DIR}/parquet/unsw_nb15_preprocessed\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        results['unsw_nb15'] = {'status': 'FAILED', 'error': str(e)}\n",
    "        print(f\"‚ùå UNSW-NB15 failed: {e}\")\n",
    "else:\n",
    "    results['unsw_nb15'] = {'status': 'SKIPPED', 'reason': 'Already processed'}\n",
    "    print(\"‚è≠Ô∏è Skipping UNSW-NB15 (already processed)\")\n",
    "\n",
    "# ===== SUMMARY =====\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä PREPROCESSING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "for dataset, result in results.items():\n",
    "    status = result['status']\n",
    "    print(f\"\\n{dataset.upper()}: {status}\")\n",
    "    if status == 'SUCCESS':\n",
    "        print(f\"  ‚è±Ô∏è  Time: {result['time']}\")\n",
    "        print(f\"  üè∑Ô∏è  Class Weights: {result['class_weights']}\")\n",
    "        print(f\"  üìê Features: {result['num_features']}\")\n",
    "    elif status == 'FAILED':\n",
    "        print(f\"  ‚ùå Error: {result['error']}\")\n",
    "    elif status == 'SKIPPED':\n",
    "        print(f\"  ‚è≠Ô∏è  {result['reason']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ All available datasets processed!\")\n",
    "print(f\"üìÅ Output location: {OUTPUT_DIR}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20379363",
   "metadata": {},
   "source": [
    "## Step 8: Verify Output Files\n",
    "\n",
    "Check the generated parquet files and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e165ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List output files\n",
    "import os\n",
    "\n",
    "print(\"üìÅ Generated Files:\\n\")\n",
    "\n",
    "parquet_dir = f\"{OUTPUT_DIR}/parquet\"\n",
    "model_dir = f\"{OUTPUT_DIR}/models\"\n",
    "\n",
    "if os.path.exists(parquet_dir):\n",
    "    print(\"Parquet Files:\")\n",
    "    for f in os.listdir(parquet_dir):\n",
    "        size = os.path.getsize(os.path.join(parquet_dir, f)) / (1024**2)  # MB\n",
    "        print(f\"  ‚úì {f} ({size:.2f} MB)\")\n",
    "\n",
    "if os.path.exists(model_dir):\n",
    "    print(\"\\nScaler Models:\")\n",
    "    for d in os.listdir(model_dir):\n",
    "        print(f\"  ‚úì {d}\")\n",
    "\n",
    "# Sample data from one dataset\n",
    "print(\"\\nüìä Sample Data (CIC-IDS 2017):\")\n",
    "parquet_path = f\"{OUTPUT_DIR}/parquet/cicids2017_preprocessed.parquet\"\n",
    "if os.path.exists(parquet_path):\n",
    "    sample_df = spark.read.parquet(parquet_path)\n",
    "    print(f\"  Total Records: {sample_df.count():,}\")\n",
    "    print(f\"  Columns: {len(sample_df.columns)}\")\n",
    "    print(f\"\\n  Schema:\")\n",
    "    sample_df.printSchema()\n",
    "    print(f\"\\n  Sample Rows:\")\n",
    "    sample_df.select('binary_label', 'sample_weight').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd904eb6",
   "metadata": {},
   "source": [
    "## Step 9: Download Results (Google Colab Only)\n",
    "\n",
    "If running on Google Colab, download the preprocessed files to your local machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317cf904",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import files\n",
    "    import shutil\n",
    "    \n",
    "    print(\"üì¶ Creating download archive...\")\n",
    "    \n",
    "    # Create zip of output directory\n",
    "    shutil.make_archive('/content/preprocessing_output', 'zip', OUTPUT_DIR)\n",
    "    \n",
    "    print(\"‚¨áÔ∏è  Downloading preprocessing_output.zip...\")\n",
    "    files.download('/content/preprocessing_output.zip')\n",
    "    \n",
    "    print(\"‚úÖ Download complete!\")\n",
    "    print(\"\\nThe zip contains:\")\n",
    "    print(\"  ‚Ä¢ parquet/ - Preprocessed datasets in Parquet format\")\n",
    "    print(\"  ‚Ä¢ models/ - StandardScaler models for each dataset\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"‚ÑπÔ∏è  Not running on Google Colab - files are already saved locally at:\")\n",
    "    print(f\"   {OUTPUT_DIR}\")\n",
    "    print(\"\\nTo download from Colab, uncomment and run this cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58edd33",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "### Using Preprocessed Data\n",
    "\n",
    "Once preprocessing is complete, you can use the parquet files for training:\n",
    "\n",
    "```python\n",
    "# Load preprocessed data\n",
    "df = spark.read.parquet(f\"{OUTPUT_DIR}/parquet/cicids2017_preprocessed.parquet\")\n",
    "\n",
    "# Use sample_weight column for training\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    featuresCol='scaled_features',\n",
    "    labelCol='binary_label',\n",
    "    weightCol='sample_weight',  # ‚Üê Handles class imbalance\n",
    "    numTrees=100\n",
    ")\n",
    "\n",
    "model = rf.fit(df)\n",
    "```\n",
    "\n",
    "### Output Structure\n",
    "\n",
    "```\n",
    "output/\n",
    "‚îú‚îÄ‚îÄ parquet/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ cicids2017_preprocessed.parquet\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ cicids2018_preprocessed.parquet\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ unsw_nb15_preprocessed.parquet\n",
    "‚îî‚îÄ‚îÄ models/\n",
    "    ‚îú‚îÄ‚îÄ cicids2017_scaler/\n",
    "    ‚îú‚îÄ‚îÄ cicids2018_scaler/\n",
    "    ‚îî‚îÄ‚îÄ unsw_nb15_scaler/\n",
    "```\n",
    "\n",
    "### Class Imbalance Handling\n",
    "\n",
    "Sample weights are calculated as: `weight = max_count / class_count`\n",
    "\n",
    "This ensures minority classes (attacks) have higher weights during training, preventing the model from becoming biased toward the majority class (benign traffic)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
