{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "92b7a78a",
      "metadata": {
        "id": "92b7a78a"
      },
      "source": [
        "## Step 1: Install PySpark and Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50b62aba",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50b62aba",
        "outputId": "c6b1f83f-733e-409d-ea6d-eb7c22cfe765"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m434.2/434.2 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m203.0/203.0 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyspark (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "‚úÖ Libraries imported successfully\n"
          ]
        }
      ],
      "source": [
        "# Install PySpark (Colab only)\n",
        "!pip install pyspark -q\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.ml.classification import RandomForestClassifier, GBTClassifier\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "from pyspark.mllib.evaluation import MulticlassMetrics\n",
        "import time\n",
        "import json\n",
        "import os\n",
        "import gc  # Add this import\n",
        "\n",
        "print(\"‚úÖ Libraries imported successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1d08265",
      "metadata": {
        "id": "f1d08265"
      },
      "source": [
        "## Step 2: Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1cfdeb5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1cfdeb5",
        "outputId": "3c99f032-824a-496c-d7ce-b326a21391ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "‚úÖ Google Drive mounted successfully!\n",
            "üìÇ Data path: /content/drive/MyDrive/NetworkIDS/output/parquet/cicids_merged_harmonized\n",
            "üìÇ Model directory: /content/drive/MyDrive/NetworkIDS/output/models\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    BASE_DIR = \"/content/drive/MyDrive/NetworkIDS\"\n",
        "    print(f\"‚úÖ Google Drive mounted successfully!\")\n",
        "    IS_COLAB = True\n",
        "except:\n",
        "    BASE_DIR = \"d:/Coding/real-time-network-intrusion-detection-spark-kafka/data\"\n",
        "    print(f\"‚úÖ Running locally. Data directory: {BASE_DIR}\")\n",
        "    IS_COLAB = False\n",
        "\n",
        "# Define paths\n",
        "DATA_PATH = f\"{BASE_DIR}/output/parquet/cicids_merged_harmonized\"\n",
        "MODEL_DIR = f\"{BASE_DIR}/output/models\"\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"üìÇ Data path: {DATA_PATH}\")\n",
        "print(f\"üìÇ Model directory: {MODEL_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c956f259",
      "metadata": {
        "id": "c956f259"
      },
      "source": [
        "## Step 3: Create Spark Session"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"--driver-memory 8g --executor-memory 8g pyspark-shell\"\n"
      ],
      "metadata": {
        "id": "ZFA8h3mABPJJ"
      },
      "id": "ZFA8h3mABPJJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88bf381d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88bf381d",
        "outputId": "0b9e820d-e489-4d40-d273-65fa5f9e6596"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Spark session created\n",
            "üìä Spark version: 4.0.1\n",
            "üîß Using 2 cores to reduce memory pressure\n"
          ]
        }
      ],
      "source": [
        "# Create Spark session optimized for ML training with better stability\n",
        "import gc\n",
        "\n",
        "# Force garbage collection before creating session\n",
        "gc.collect()\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"NIDS-ModelTraining\") \\\n",
        "    .config(\"spark.driver.memory\", \"8g\") \\\n",
        "    .config(\"spark.executor.memory\", \"8g\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"100\") \\\n",
        "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
        "    .config(\"spark.sql.parquet.enableVectorizedReader\", \"false\") \\\n",
        "    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
        "    .config(\"spark.network.timeout\", \"800s\") \\\n",
        "    .config(\"spark.executor.heartbeatInterval\", \"60s\") \\\n",
        "    .config(\"spark.sql.broadcastTimeout\", \"600\") \\\n",
        "    .config(\"spark.rpc.askTimeout\", \"600s\") \\\n",
        "    .config(\"spark.storage.memoryFraction\", \"0.5\") \\\n",
        "    .config(\"spark.memory.fraction\", \"0.6\") \\\n",
        "    .master(\"local[2]\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "spark.sparkContext.setLogLevel(\"ERROR\")\n",
        "print(f\"‚úÖ Spark session created\")\n",
        "print(f\"üìä Spark version: {spark.version}\")\n",
        "print(f\"üîß Using 2 cores to reduce memory pressure\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d7c6a8d",
      "metadata": {
        "id": "7d7c6a8d"
      },
      "source": [
        "## Step 4: Load Harmonized Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c867ab8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c867ab8",
        "outputId": "cf15170a-2f42-4b85-99a9-32d4c77e3e69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading harmonized dataset...\n",
            "‚úÖ Dataset loaded in 4.62 seconds\n",
            "üìä Columns: 34\n",
            "\n",
            "Schema (key columns):\n",
            "  - features_scaled: VectorUDT()\n",
            "  - binary_label: IntegerType()\n",
            "  - unified_label: IntegerType()\n",
            "  - sample_weight: DoubleType()\n",
            "  - multiclass_weight: DoubleType()\n"
          ]
        }
      ],
      "source": [
        "# Load the harmonized dataset\n",
        "print(\"Loading harmonized dataset...\")\n",
        "start_time = time.time()\n",
        "\n",
        "df = spark.read.parquet(DATA_PATH)\n",
        "\n",
        "# Show basic info\n",
        "print(f\"‚úÖ Dataset loaded in {time.time() - start_time:.2f} seconds\")\n",
        "print(f\"üìä Columns: {len(df.columns)}\")\n",
        "print(f\"\\nSchema (key columns):\")\n",
        "for col in ['features_scaled', 'binary_label', 'unified_label', 'sample_weight', 'multiclass_weight']:\n",
        "    if col in df.columns:\n",
        "        print(f\"  - {col}: {df.schema[col].dataType}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6835c19",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6835c19",
        "outputId": "0ac49d81-d5f2-416f-a099-3cd767fbe4aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Binary Label Distribution:\n",
            "+------------+--------+\n",
            "|binary_label|   count|\n",
            "+------------+--------+\n",
            "|           1| 2779281|\n",
            "|           0|15484134|\n",
            "+------------+--------+\n",
            "\n",
            "\n",
            "Unified Label Distribution:\n",
            "+-------------+--------+\n",
            "|unified_label|   count|\n",
            "+-------------+--------+\n",
            "|            0|15484134|\n",
            "|            1|  699820|\n",
            "|            2|  705921|\n",
            "|            3|  165820|\n",
            "|            4|     928|\n",
            "|            5|  161095|\n",
            "|            6|  284263|\n",
            "|            7|   90819|\n",
            "|            8|  670615|\n",
            "+-------------+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Check label distributions\n",
        "print(\"Binary Label Distribution:\")\n",
        "df.groupBy('binary_label').count().show()\n",
        "\n",
        "print(\"\\nUnified Label Distribution:\")\n",
        "df.groupBy('unified_label').count().orderBy('unified_label').show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e575726f",
      "metadata": {
        "id": "e575726f"
      },
      "source": [
        "## Step 5: Prepare Data for Training\n",
        "\n",
        "We'll use stratified sampling to maintain class distribution in train/test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f742ad70",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f742ad70",
        "outputId": "7706f35d-f553-4b18-bf7d-089a3ceda668"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Selected 5 columns for training\n",
            "root\n",
            " |-- features_scaled: vector (nullable = true)\n",
            " |-- binary_label: integer (nullable = true)\n",
            " |-- unified_label: integer (nullable = true)\n",
            " |-- sample_weight: double (nullable = true)\n",
            " |-- multiclass_weight: double (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Select only needed columns for training (reduces memory)\n",
        "df_train = df.select(\n",
        "    'features_scaled',\n",
        "    'binary_label',\n",
        "    'unified_label',\n",
        "    'sample_weight',\n",
        "    'multiclass_weight'\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Selected {len(df_train.columns)} columns for training\")\n",
        "df_train.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ffa149b",
      "metadata": {
        "id": "4ffa149b"
      },
      "outputs": [],
      "source": [
        "# === SPARK HEALTH CHECK - Run before each training ===\n",
        "def ensure_spark_active():\n",
        "    \"\"\"Ensure Spark session is active, recreate if needed\"\"\"\n",
        "    global spark, train_sampled, test_sampled\n",
        "    try:\n",
        "        # Test if Spark is alive\n",
        "        spark.sparkContext.parallelize([1,2,3]).count()\n",
        "        print(\"‚úÖ Spark session is active\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Spark session dead: {e}\")\n",
        "        print(\"üîÑ Recreating Spark session...\")\n",
        "\n",
        "        try:\n",
        "            spark.stop()\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        gc.collect()\n",
        "\n",
        "        spark = SparkSession.builder \\\n",
        "            .appName(\"NIDS-ModelTraining\") \\\n",
        "            .config(\"spark.driver.memory\", \"8g\") \\\n",
        "            .config(\"spark.executor.memory\", \"8g\") \\\n",
        "            .config(\"spark.sql.shuffle.partitions\", \"100\") \\\n",
        "            .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
        "            .config(\"spark.sql.parquet.enableVectorizedReader\", \"false\") \\\n",
        "            .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
        "            .config(\"spark.network.timeout\", \"800s\") \\\n",
        "            .config(\"spark.executor.heartbeatInterval\", \"60s\") \\\n",
        "            .config(\"spark.memory.fraction\", \"0.6\") \\\n",
        "            .master(\"local[2]\") \\\n",
        "            .getOrCreate()\n",
        "\n",
        "        spark.sparkContext.setLogLevel(\"ERROR\")\n",
        "\n",
        "        # Reload data after session recreation\n",
        "        print(\"üîÑ Reloading training data...\")\n",
        "        df = spark.read.parquet(DATA_PATH)\n",
        "        df_train = df.select(\n",
        "            'features_scaled', 'binary_label', 'unified_label',\n",
        "            'sample_weight', 'multiclass_weight'\n",
        "        )\n",
        "        train_df, test_df = df_train.randomSplit([0.8, 0.2], seed=42)\n",
        "        train_sampled = train_df.sample(fraction=0.15, seed=42).cache()\n",
        "        test_sampled = test_df.sample(fraction=0.15, seed=42).cache()\n",
        "        train_sampled.count()  # Materialize\n",
        "        test_sampled.count()\n",
        "\n",
        "        print(\"‚úÖ New Spark session created and data reloaded\")\n",
        "        return True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into train/test sets (80/20 split)\n",
        "print(\"Splitting data into train/test sets...\")\n",
        "\n",
        "# Perform stratified split on binary_label\n",
        "train_df, test_df = df_train.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# Cache the splits\n",
        "train_df.cache()\n",
        "test_df.cache()\n",
        "\n",
        "# Get counts (triggers caching)\n",
        "train_total = train_df.count()\n",
        "test_total = test_df.count()\n",
        "\n",
        "print(f\"‚úÖ Training set: {train_total:,} records\")\n",
        "print(f\"‚úÖ Test set: {test_total:,} records\")\n",
        "print(f\"üìä Train/Test ratio: {train_total/(train_total+test_total)*100:.1f}% / {test_total/(train_total+test_total)*100:.1f}%\")\n",
        "\n",
        "# Unpersist the original df to free memory\n",
        "df.unpersist()\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c-ZsaBPlfk_",
        "outputId": "ac786f6d-60ee-493d-bf3d-2e62eecfe8e2"
      },
      "id": "2c-ZsaBPlfk_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Splitting data into train/test sets...\n",
            "‚úÖ Training set: 14,608,562 records\n",
            "‚úÖ Test set: 3,654,853 records\n",
            "üìä Train/Test ratio: 80.0% / 20.0%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data to reduce memory (KEY FIX for Colab's 12GB limit)\n",
        "print(\"üìä Sampling data to fit in memory...\")\n",
        "SAMPLE_FRACTION = 0.15  # Use 15% of data\n",
        "\n",
        "# Unpersist full datasets before sampling\n",
        "train_df.unpersist()\n",
        "test_df.unpersist()\n",
        "gc.collect()\n",
        "\n",
        "train_sampled = train_df.sample(fraction=SAMPLE_FRACTION, seed=42)\n",
        "test_sampled = test_df.sample(fraction=SAMPLE_FRACTION, seed=42)\n",
        "\n",
        "# Cache sampled data\n",
        "train_sampled.cache()\n",
        "test_sampled.cache()\n",
        "\n",
        "# Force materialization\n",
        "train_count = train_sampled.count()\n",
        "test_count = test_sampled.count()\n",
        "\n",
        "print(f\"‚úÖ Training set: {train_count:,} records (sampled)\")\n",
        "print(f\"‚úÖ Test set: {test_count:,} records (sampled)\")\n",
        "print(f\"üí° Using {SAMPLE_FRACTION*100:.0f}% sample to prevent OOM crashes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmnPMhRWiWNs",
        "outputId": "87769832-3d96-4d35-b0a4-39c58ba19499"
      },
      "id": "pmnPMhRWiWNs",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Sampling data to fit in memory...\n",
            "‚úÖ Training set: 2,191,961 records (sampled)\n",
            "‚úÖ Test set: 548,492 records (sampled)\n",
            "üí° Using 15% sample to prevent OOM crashes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08a8d54b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08a8d54b",
        "outputId": "9ff317ed-103e-4c8b-c477-2ec6fc70f6ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample from train set:\n",
            "+------------+-------------+\n",
            "|binary_label|unified_label|\n",
            "+------------+-------------+\n",
            "|           0|            0|\n",
            "|           0|            0|\n",
            "|           0|            0|\n",
            "|           0|            0|\n",
            "|           0|            0|\n",
            "+------------+-------------+\n",
            "only showing top 5 rows\n",
            "‚úÖ Data ready for training\n"
          ]
        }
      ],
      "source": [
        "# Quick verification - just show sample, skip expensive groupBy on full data\n",
        "print(\"Sample from train set:\")\n",
        "train_df.select('binary_label', 'unified_label').show(5)\n",
        "\n",
        "print(\"‚úÖ Data ready for training\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "564f6661",
      "metadata": {
        "id": "564f6661"
      },
      "source": [
        "## Step 6: Train Binary Classification Models\n",
        "\n",
        "### 6.1 Random Forest - Binary Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7915e877",
      "metadata": {
        "id": "7915e877",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa1bac3e-975d-46d8-cbd4-21f28e70f989"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Training Random Forest - Binary Classification\n",
            "============================================================\n",
            "‚úÖ Spark session is active\n",
            "Training model on sampled data...\n",
            "‚úÖ Training completed in 0.72 minutes\n"
          ]
        }
      ],
      "source": [
        "# Random Forest for Binary Classification - WITH RECOVERY\n",
        "print(\"=\"*60)\n",
        "print(\"Training Random Forest - Binary Classification\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Ensure Spark is alive before training\n",
        "ensure_spark_active()\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "rf_binary = RandomForestClassifier(\n",
        "    featuresCol='features_scaled',\n",
        "    labelCol='binary_label',\n",
        "    weightCol='sample_weight',\n",
        "    numTrees=30,       # Reduced for stability\n",
        "    maxDepth=6,        # Reduced for stability\n",
        "    maxBins=32,\n",
        "    minInstancesPerNode=10,  # Prevent overfitting, speeds up\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "print(\"Training model on sampled data...\")\n",
        "try:\n",
        "    rf_binary_model = rf_binary.fit(train_sampled)\n",
        "    elapsed = time.time() - start_time\n",
        "    print(f\"‚úÖ Training completed in {elapsed/60:.2f} minutes\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Training failed: {e}\")\n",
        "    print(\"üí° Try reducing SAMPLE_FRACTION or numTrees\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70b75119",
      "metadata": {
        "id": "70b75119",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be274696-ce9f-4ed6-9d61-7b8769fa57b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating Random Forest - Binary Classification...\n",
            "‚úÖ Spark session is active\n",
            "\n",
            "==================================================\n",
            "Random Forest - Binary Classification Results\n",
            "==================================================\n",
            "AUC-ROC:   0.9757\n",
            "AUC-PR:    0.9414\n",
            "Accuracy:  0.9715\n",
            "F1 Score:  0.9719\n",
            "Precision: 0.9728\n",
            "Recall:    0.9715\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "732"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# Evaluate Random Forest - Binary\n",
        "print(\"Evaluating Random Forest - Binary Classification...\")\n",
        "\n",
        "ensure_spark_active()\n",
        "\n",
        "# Predictions on sampled test set\n",
        "rf_binary_preds = rf_binary_model.transform(test_sampled)\n",
        "\n",
        "# Binary metrics\n",
        "binary_evaluator_auc = BinaryClassificationEvaluator(\n",
        "    labelCol='binary_label',\n",
        "    rawPredictionCol='rawPrediction',\n",
        "    metricName='areaUnderROC'\n",
        ")\n",
        "\n",
        "binary_evaluator_pr = BinaryClassificationEvaluator(\n",
        "    labelCol='binary_label',\n",
        "    rawPredictionCol='rawPrediction',\n",
        "    metricName='areaUnderPR'\n",
        ")\n",
        "\n",
        "multi_evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol='binary_label',\n",
        "    predictionCol='prediction'\n",
        ")\n",
        "\n",
        "auc_roc = binary_evaluator_auc.evaluate(rf_binary_preds)\n",
        "auc_pr = binary_evaluator_pr.evaluate(rf_binary_preds)\n",
        "accuracy = multi_evaluator.evaluate(rf_binary_preds, {multi_evaluator.metricName: 'accuracy'})\n",
        "f1 = multi_evaluator.evaluate(rf_binary_preds, {multi_evaluator.metricName: 'f1'})\n",
        "precision = multi_evaluator.evaluate(rf_binary_preds, {multi_evaluator.metricName: 'weightedPrecision'})\n",
        "recall = multi_evaluator.evaluate(rf_binary_preds, {multi_evaluator.metricName: 'weightedRecall'})\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Random Forest - Binary Classification Results\")\n",
        "print(\"=\"*50)\n",
        "print(f\"AUC-ROC:   {auc_roc:.4f}\")\n",
        "print(f\"AUC-PR:    {auc_pr:.4f}\")\n",
        "print(f\"Accuracy:  {accuracy:.4f}\")\n",
        "print(f\"F1 Score:  {f1:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall:    {recall:.4f}\")\n",
        "\n",
        "rf_binary_results = {\n",
        "    'model': 'Random Forest',\n",
        "    'task': 'Binary Classification',\n",
        "    'auc_roc': auc_roc,\n",
        "    'auc_pr': auc_pr,\n",
        "    'accuracy': accuracy,\n",
        "    'f1': f1,\n",
        "    'precision': precision,\n",
        "    'recall': recall\n",
        "}\n",
        "\n",
        "# Clean up predictions to free memory\n",
        "rf_binary_preds.unpersist()\n",
        "import gc\n",
        "gc.collect()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a634cc90",
      "metadata": {
        "id": "a634cc90"
      },
      "source": [
        "### 6.2 Gradient Boosted Trees - Binary Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e7fb591",
      "metadata": {
        "id": "2e7fb591",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a6fa0d1-e20a-49b8-ab86-7b2f2d8728d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Training Gradient Boosted Trees - Binary Classification\n",
            "============================================================\n",
            "‚úÖ Spark session is active\n",
            "Training model on sampled data...\n",
            "‚úÖ Training completed in 1.46 minutes\n"
          ]
        }
      ],
      "source": [
        "# GBT for Binary Classification - WITH RECOVERY\n",
        "print(\"=\"*60)\n",
        "print(\"Training Gradient Boosted Trees - Binary Classification\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "ensure_spark_active()\n",
        "gc.collect()\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "gbt_binary = GBTClassifier(\n",
        "    featuresCol='features_scaled',\n",
        "    labelCol='binary_label',\n",
        "    weightCol='sample_weight',\n",
        "    maxIter=20,        # Reduced for stability\n",
        "    maxDepth=5,        # Reduced for stability\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "print(\"Training model on sampled data...\")\n",
        "try:\n",
        "    gbt_binary_model = gbt_binary.fit(train_sampled)\n",
        "    elapsed = time.time() - start_time\n",
        "    print(f\"‚úÖ Training completed in {elapsed/60:.2f} minutes\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Training failed: {e}\")\n",
        "    print(\"üí° Try reducing maxIter or maxDepth\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5225540",
      "metadata": {
        "id": "a5225540",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfba0762-f11e-4e72-987d-159634148a9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating GBT - Binary Classification...\n",
            "‚úÖ Spark session is active\n",
            "\n",
            "==================================================\n",
            "GBT - Binary Classification Results\n",
            "==================================================\n",
            "AUC-ROC:   0.9852\n",
            "AUC-PR:    0.9713\n",
            "Accuracy:  0.9817\n",
            "F1 Score:  0.9818\n",
            "Precision: 0.9819\n",
            "Recall:    0.9817\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "336"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# Evaluate GBT - Binary\n",
        "print(\"Evaluating GBT - Binary Classification...\")\n",
        "\n",
        "ensure_spark_active()\n",
        "\n",
        "gbt_binary_preds = gbt_binary_model.transform(test_sampled)\n",
        "\n",
        "auc_roc = binary_evaluator_auc.evaluate(gbt_binary_preds)\n",
        "auc_pr = binary_evaluator_pr.evaluate(gbt_binary_preds)\n",
        "accuracy = multi_evaluator.evaluate(gbt_binary_preds, {multi_evaluator.metricName: 'accuracy'})\n",
        "f1 = multi_evaluator.evaluate(gbt_binary_preds, {multi_evaluator.metricName: 'f1'})\n",
        "precision = multi_evaluator.evaluate(gbt_binary_preds, {multi_evaluator.metricName: 'weightedPrecision'})\n",
        "recall = multi_evaluator.evaluate(gbt_binary_preds, {multi_evaluator.metricName: 'weightedRecall'})\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"GBT - Binary Classification Results\")\n",
        "print(\"=\"*50)\n",
        "print(f\"AUC-ROC:   {auc_roc:.4f}\")\n",
        "print(f\"AUC-PR:    {auc_pr:.4f}\")\n",
        "print(f\"Accuracy:  {accuracy:.4f}\")\n",
        "print(f\"F1 Score:  {f1:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall:    {recall:.4f}\")\n",
        "\n",
        "gbt_binary_results = {\n",
        "    'model': 'Gradient Boosted Trees',\n",
        "    'task': 'Binary Classification',\n",
        "    'auc_roc': auc_roc,\n",
        "    'auc_pr': auc_pr,\n",
        "    'accuracy': accuracy,\n",
        "    'f1': f1,\n",
        "    'precision': precision,\n",
        "    'recall': recall\n",
        "}\n",
        "\n",
        "# Cleanup\n",
        "gbt_binary_preds.unpersist()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6819415",
      "metadata": {
        "id": "e6819415"
      },
      "source": [
        "## Step 7: Train Multi-class Classification Models\n",
        "\n",
        "### 7.1 Random Forest - Multi-class (9 attack types)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91c07d9c",
      "metadata": {
        "id": "91c07d9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c242a7bd-aa55-4ce8-cc68-c7fe46df110c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Training Random Forest - Multi-class Classification (9 classes)\n",
            "============================================================\n",
            "‚úÖ Spark session is active\n",
            "Training model on sampled data...\n",
            "‚úÖ Training completed in 0.96 minutes\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Random Forest for Multi-class Classification - WITH RECOVERY & OPTIMIZATION\n",
        "print(\"=\"*60)\n",
        "print(\"Training Random Forest - Multi-class Classification (9 classes)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "ensure_spark_active()\n",
        "gc.collect()\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "rf_multi = RandomForestClassifier(\n",
        "    featuresCol='features_scaled',\n",
        "    labelCol='unified_label',\n",
        "    weightCol='multiclass_weight',\n",
        "    numTrees=30,           # Reduced from 50\n",
        "    maxDepth=8,            # Reduced from 10\n",
        "    maxBins=32,\n",
        "    minInstancesPerNode=10,  # Prevents overfitting, speeds training\n",
        "    minInfoGain=0.001,       # Skip splits with minimal gain\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "print(\"Training model on sampled data...\")\n",
        "try:\n",
        "    rf_multi_model = rf_multi.fit(train_sampled)\n",
        "    elapsed = time.time() - start_time\n",
        "    print(f\"‚úÖ Training completed in {elapsed/60:.2f} minutes\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Training failed: {e}\")\n",
        "    print(\"üí° Try reducing numTrees to 20 or maxDepth to 6\")\n",
        "    raise\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8beadf61",
      "metadata": {
        "id": "8beadf61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f39678bd-17a8-495f-a96a-8a667d8b0d63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating Random Forest - Multi-class Classification...\n",
            "‚úÖ Spark session is active\n",
            "\n",
            "==================================================\n",
            "Random Forest - Multi-class Classification Results\n",
            "==================================================\n",
            "Accuracy:  0.5573\n",
            "F1 Score:  0.6813\n",
            "Precision: 0.9666\n",
            "Recall:    0.5573\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "90"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "# Evaluate Random Forest - Multi-class\n",
        "print(\"Evaluating Random Forest - Multi-class Classification...\")\n",
        "\n",
        "ensure_spark_active()\n",
        "\n",
        "rf_multi_preds = rf_multi_model.transform(test_sampled)\n",
        "\n",
        "# Create evaluator specifically for multi-class with unified_label\n",
        "mc_evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol='unified_label',\n",
        "    predictionCol='prediction'\n",
        ")\n",
        "\n",
        "accuracy = mc_evaluator.evaluate(rf_multi_preds, {mc_evaluator.metricName: 'accuracy'})\n",
        "f1 = mc_evaluator.evaluate(rf_multi_preds, {mc_evaluator.metricName: 'f1'})\n",
        "precision = mc_evaluator.evaluate(rf_multi_preds, {mc_evaluator.metricName: 'weightedPrecision'})\n",
        "recall = mc_evaluator.evaluate(rf_multi_preds, {mc_evaluator.metricName: 'weightedRecall'})\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Random Forest - Multi-class Classification Results\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Accuracy:  {accuracy:.4f}\")\n",
        "print(f\"F1 Score:  {f1:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall:    {recall:.4f}\")\n",
        "\n",
        "rf_multi_results = {\n",
        "    'model': 'Random Forest',\n",
        "    'task': 'Multi-class Classification (9 classes)',\n",
        "    'accuracy': accuracy,\n",
        "    'f1': f1,\n",
        "    'precision': precision,\n",
        "    'recall': recall\n",
        "}\n",
        "\n",
        "# Cleanup\n",
        "rf_multi_preds.unpersist()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fbd1453",
      "metadata": {
        "id": "2fbd1453",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7ad8e05-c50a-4c14-862b-f3e600b90f1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generating Confusion Matrix...\n",
            "‚úÖ Spark session is active\n",
            "Per-class Prediction Accuracy:\n",
            "+-------------+------+-------+--------+\n",
            "|unified_label| total|correct|accuracy|\n",
            "+-------------+------+-------+--------+\n",
            "|            0|464986| 224359|  0.4825|\n",
            "|            1| 21029|  20331|  0.9668|\n",
            "|            2| 21303|  21265|  0.9982|\n",
            "|            3|  4933|   4929|  0.9992|\n",
            "|            4|    30|     27|     0.9|\n",
            "|            5|  4867|   3915|  0.8044|\n",
            "|            6|  8539|   8478|  0.9929|\n",
            "|            7|  2686|   2677|  0.9966|\n",
            "|            8| 20119|  19687|  0.9785|\n",
            "+-------------+------+-------+--------+\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "395"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "\n",
        "# Confusion Matrix for Multi-class (regenerate predictions for display)\n",
        "print(\"\\nGenerating Confusion Matrix...\")\n",
        "ensure_spark_active()\n",
        "\n",
        "rf_multi_preds = rf_multi_model.transform(test_sampled)\n",
        "\n",
        "print(\"Per-class Prediction Accuracy:\")\n",
        "rf_multi_preds.groupBy('unified_label') \\\n",
        "    .agg(\n",
        "        F.count('*').alias('total'),\n",
        "        F.sum(F.when(F.col('prediction') == F.col('unified_label'), 1).otherwise(0)).alias('correct')\n",
        "    ) \\\n",
        "    .withColumn('accuracy', F.round(F.col('correct') / F.col('total'), 4)) \\\n",
        "    .orderBy('unified_label').show()\n",
        "\n",
        "rf_multi_preds.unpersist()\n",
        "gc.collect()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "737c7e15",
      "metadata": {
        "id": "737c7e15"
      },
      "source": [
        "## Step 8: Save Trained Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c4b85c7",
      "metadata": {
        "id": "0c4b85c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9eb07bd1-96ce-482b-c218-4e77664ba57e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Google Drive connection verified\n"
          ]
        }
      ],
      "source": [
        "# Re-verify Google Drive connection before saving\n",
        "if IS_COLAB:\n",
        "    try:\n",
        "        os.listdir(BASE_DIR)\n",
        "        print(\"‚úÖ Google Drive connection verified\")\n",
        "    except:\n",
        "        print(\"‚ö†Ô∏è Drive disconnected! Remounting...\")\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive', force_remount=True)\n",
        "        print(\"‚úÖ Drive remounted successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a30de86a",
      "metadata": {
        "id": "a30de86a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5df87474-9562-4197-a6ff-0a47c98f4ebb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving trained models...\n",
            "‚úÖ Saved: /content/drive/MyDrive/NetworkIDS/output/models/rf_binary_classifier\n",
            "‚úÖ Saved: /content/drive/MyDrive/NetworkIDS/output/models/gbt_binary_classifier\n",
            "‚úÖ Saved: /content/drive/MyDrive/NetworkIDS/output/models/rf_multiclass_classifier\n"
          ]
        }
      ],
      "source": [
        "# Save models\n",
        "print(\"Saving trained models...\")\n",
        "\n",
        "# Save Random Forest - Binary\n",
        "rf_binary_path = f\"{MODEL_DIR}/rf_binary_classifier\"\n",
        "rf_binary_model.write().overwrite().save(rf_binary_path)\n",
        "print(f\"‚úÖ Saved: {rf_binary_path}\")\n",
        "\n",
        "# Save GBT - Binary\n",
        "gbt_binary_path = f\"{MODEL_DIR}/gbt_binary_classifier\"\n",
        "gbt_binary_model.write().overwrite().save(gbt_binary_path)\n",
        "print(f\"‚úÖ Saved: {gbt_binary_path}\")\n",
        "\n",
        "# Save Random Forest - Multi-class\n",
        "rf_multi_path = f\"{MODEL_DIR}/rf_multiclass_classifier\"\n",
        "rf_multi_model.write().overwrite().save(rf_multi_path)\n",
        "print(f\"‚úÖ Saved: {rf_multi_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c88747a",
      "metadata": {
        "id": "0c88747a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86c9a59b-a9a3-4259-ae9b-7cc364847b7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Results saved to: /content/drive/MyDrive/NetworkIDS/output/models/training_results.json\n"
          ]
        }
      ],
      "source": [
        "# Save training results summary\n",
        "all_results = {\n",
        "    'rf_binary': rf_binary_results,\n",
        "    'gbt_binary': gbt_binary_results,\n",
        "    'rf_multiclass': rf_multi_results,\n",
        "    'train_size': train_count,\n",
        "    'test_size': test_count,\n",
        "    'total_records': train_count + test_count\n",
        "}\n",
        "\n",
        "results_path = f\"{MODEL_DIR}/training_results.json\"\n",
        "with open(results_path, 'w') as f:\n",
        "    json.dump(all_results, f, indent=2)\n",
        "\n",
        "print(f\"\\n‚úÖ Results saved to: {results_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c01c458",
      "metadata": {
        "id": "1c01c458"
      },
      "source": [
        "## Step 9: Model Comparison Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b205b2c",
      "metadata": {
        "id": "4b205b2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "130574af-99dd-43ac-9b7a-ace30c1b91f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "MODEL TRAINING SUMMARY\n",
            "======================================================================\n",
            "\n",
            "üìä BINARY CLASSIFICATION (Attack vs Benign)\n",
            "----------------------------------------------------------------------\n",
            "Model                     AUC-ROC    Accuracy   F1         Precision  Recall    \n",
            "----------------------------------------------------------------------\n",
            "Random Forest             0.9757     0.9715     0.9719     0.9728     0.9715    \n",
            "Gradient Boosted Trees    0.9852     0.9817     0.9818     0.9819     0.9817    \n",
            "\n",
            "üìä MULTI-CLASS CLASSIFICATION (9 Attack Types)\n",
            "----------------------------------------------------------------------\n",
            "Model                     Accuracy   F1         Precision  Recall    \n",
            "----------------------------------------------------------------------\n",
            "Random Forest             0.5573     0.6813     0.9666     0.5573    \n",
            "\n",
            "======================================================================\n",
            "‚úÖ All models trained and saved successfully!\n",
            "üìÅ Models location: /content/drive/MyDrive/NetworkIDS/output/models\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# Print final comparison\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"MODEL TRAINING SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\nüìä BINARY CLASSIFICATION (Attack vs Benign)\")\n",
        "print(\"-\"*70)\n",
        "print(f\"{'Model':<25} {'AUC-ROC':<10} {'Accuracy':<10} {'F1':<10} {'Precision':<10} {'Recall':<10}\")\n",
        "print(\"-\"*70)\n",
        "print(f\"{'Random Forest':<25} {rf_binary_results['auc_roc']:<10.4f} {rf_binary_results['accuracy']:<10.4f} {rf_binary_results['f1']:<10.4f} {rf_binary_results['precision']:<10.4f} {rf_binary_results['recall']:<10.4f}\")\n",
        "print(f\"{'Gradient Boosted Trees':<25} {gbt_binary_results['auc_roc']:<10.4f} {gbt_binary_results['accuracy']:<10.4f} {gbt_binary_results['f1']:<10.4f} {gbt_binary_results['precision']:<10.4f} {gbt_binary_results['recall']:<10.4f}\")\n",
        "\n",
        "print(\"\\nüìä MULTI-CLASS CLASSIFICATION (9 Attack Types)\")\n",
        "print(\"-\"*70)\n",
        "print(f\"{'Model':<25} {'Accuracy':<10} {'F1':<10} {'Precision':<10} {'Recall':<10}\")\n",
        "print(\"-\"*70)\n",
        "print(f\"{'Random Forest':<25} {rf_multi_results['accuracy']:<10.4f} {rf_multi_results['f1']:<10.4f} {rf_multi_results['precision']:<10.4f} {rf_multi_results['recall']:<10.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚úÖ All models trained and saved successfully!\")\n",
        "print(f\"üìÅ Models location: {MODEL_DIR}\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c433532f",
      "metadata": {
        "id": "c433532f"
      },
      "source": [
        "## Step 10: Feature Importance (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb35d6ec",
      "metadata": {
        "id": "bb35d6ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "377083ac-9be6-4b11-e59b-24d89c940e14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 20 Most Important Features (Random Forest - Binary):\n",
            "==================================================\n",
            "Rank   Feature Index   Importance  \n",
            "-----------------------------------\n",
            "1      0               0.081287\n",
            "2      66              0.050510\n",
            "3      35              0.048572\n",
            "4      7               0.045518\n",
            "5      20              0.037889\n",
            "6      64              0.034247\n",
            "7      25              0.032383\n",
            "8      21              0.031753\n",
            "9      40              0.029736\n",
            "10     38              0.026515\n",
            "11     63              0.026513\n",
            "12     24              0.026096\n",
            "13     5               0.026089\n",
            "14     69              0.025748\n",
            "15     52              0.024871\n",
            "16     19              0.024671\n",
            "17     13              0.021418\n",
            "18     9               0.020865\n",
            "19     46              0.020662\n",
            "20     36              0.020216\n"
          ]
        }
      ],
      "source": [
        "# Get feature importance from Random Forest\n",
        "print(\"Top 20 Most Important Features (Random Forest - Binary):\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "importances = rf_binary_model.featureImportances.toArray()\n",
        "\n",
        "# Create feature importance list\n",
        "feature_importance = [(i, imp) for i, imp in enumerate(importances)]\n",
        "feature_importance.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "print(f\"{'Rank':<6} {'Feature Index':<15} {'Importance':<12}\")\n",
        "print(\"-\"*35)\n",
        "for rank, (idx, imp) in enumerate(feature_importance[:20], 1):\n",
        "    print(f\"{rank:<6} {idx:<15} {imp:.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3be3c365",
      "metadata": {
        "id": "3be3c365"
      },
      "source": [
        "## Summary\n",
        "\n",
        "### Models Trained:\n",
        "1. **Random Forest - Binary** (`rf_binary_classifier`)\n",
        "   - Task: Attack vs Benign\n",
        "   - Use case: Quick attack detection\n",
        "\n",
        "2. **Gradient Boosted Trees - Binary** (`gbt_binary_classifier`)\n",
        "   - Task: Attack vs Benign\n",
        "   - Use case: Higher accuracy attack detection\n",
        "\n",
        "3. **Random Forest - Multi-class** (`rf_multiclass_classifier`)\n",
        "   - Task: Identify specific attack type (9 classes)\n",
        "   - Use case: Detailed threat classification\n",
        "\n",
        "### Saved Artifacts:\n",
        "- Models: `/content/drive/MyDrive/NetworkIDS/output/models/`\n",
        "- Results: `training_results.json`\n",
        "\n",
        "### Next Steps:\n",
        "1. Deploy models for real-time inference\n",
        "2. Integrate with Kafka streaming pipeline\n",
        "3. Build alerting/monitoring dashboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b7f62ed",
      "metadata": {
        "id": "1b7f62ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "966200db-4b2f-4ab5-d91b-9163e519b76f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaning up...\n",
            "‚úÖ Spark session stopped\n",
            "\n",
            "üéâ Model training complete! Ready for deployment.\n"
          ]
        }
      ],
      "source": [
        "# Cleanup\n",
        "print(\"Cleaning up...\")\n",
        "try:\n",
        "    train_sampled.unpersist()\n",
        "    test_sampled.unpersist()\n",
        "except:\n",
        "    pass\n",
        "\n",
        "gc.collect()\n",
        "spark.stop()\n",
        "print(\"‚úÖ Spark session stopped\")\n",
        "print(\"\\nüéâ Model training complete! Ready for deployment.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f659561"
      },
      "source": [
        "**Reasoning**:\n",
        "To optimize the Random Forest binary classification evaluation and prevent `SparkOutOfMemoryError`, I will replace the existing code in cell `70b75119` with the provided code that incorporates sampling for AUC-ROC and AUC-PR calculations while retaining full dataset evaluation for other metrics.\n",
        "\n"
      ],
      "id": "3f659561"
    },
    {
      "cell_type": "code",
      "source": [
        "# === RELOAD SESSION AND DATA ===\n",
        "import gc\n",
        "gc.collect()\n",
        "\n",
        "!pip install pyspark -q\n",
        "\n",
        "# Recreate Spark session\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.ml.classification import RandomForestClassifier, RandomForestClassificationModel\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "import time\n",
        "import os\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"NIDS-ModelTraining\") \\\n",
        "    .config(\"spark.driver.memory\", \"8g\") \\\n",
        "    .config(\"spark.executor.memory\", \"8g\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"100\") \\\n",
        "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
        "    .config(\"spark.sql.parquet.enableVectorizedReader\", \"false\") \\\n",
        "    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
        "    .config(\"spark.network.timeout\", \"800s\") \\\n",
        "    .config(\"spark.executor.heartbeatInterval\", \"60s\") \\\n",
        "    .config(\"spark.memory.fraction\", \"0.6\") \\\n",
        "    .master(\"local[2]\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "spark.sparkContext.setLogLevel(\"ERROR\")\n",
        "print(\"‚úÖ Spark session created\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VBIR3hVtx4y",
        "outputId": "a8cd1bde-8e2d-4489-eb49-0ac28f956468"
      },
      "id": "2VBIR3hVtx4y",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m434.2/434.2 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m203.0/203.0 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyspark (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "‚úÖ Spark session created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === MOUNT GOOGLE DRIVE FIRST ===\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"‚úÖ Google Drive mounted\")\n",
        "\n",
        "# === VERIFY DATA PATH ===\n",
        "import os\n",
        "\n",
        "DATA_PATH = \"/content/drive/MyDrive/NetworkIDS/output/parquet/cicids_merged_harmonized\"\n",
        "MODEL_DIR = \"/content/drive/MyDrive/NetworkIDS/output/models\"\n",
        "\n",
        "# Check if path exists\n",
        "if os.path.exists(DATA_PATH):\n",
        "    print(f\"‚úÖ Data path exists: {DATA_PATH}\")\n",
        "    # List contents\n",
        "    print(f\"üìÅ Contents: {os.listdir(DATA_PATH)[:5]}...\")\n",
        "else:\n",
        "    print(f\"‚ùå Data path NOT found: {DATA_PATH}\")\n",
        "    # List what's available\n",
        "    base = \"/content/drive/MyDrive/NetworkIDS\"\n",
        "    if os.path.exists(base):\n",
        "        print(f\"\\nüìÇ Available in {base}:\")\n",
        "        for item in os.listdir(base):\n",
        "            print(f\"  - {item}\")\n",
        "        output_path = f\"{base}/output\"\n",
        "        if os.path.exists(output_path):\n",
        "            print(f\"\\nüìÇ Available in {output_path}:\")\n",
        "            for item in os.listdir(output_path):\n",
        "                print(f\"  - {item}\")\n",
        "    else:\n",
        "        print(f\"‚ùå Base directory not found: {base}\")\n",
        "        print(\"\\nüìÇ Available in MyDrive:\")\n",
        "        print(os.listdir(\"/content/drive/MyDrive\")[:10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36ODin-Xt51j",
        "outputId": "7f6232f3-3674-4257-8f8b-f36084abc174"
      },
      "id": "36ODin-Xt51j",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "‚úÖ Google Drive mounted\n",
            "‚úÖ Data path exists: /content/drive/MyDrive/NetworkIDS/output/parquet/cicids_merged_harmonized\n",
            "üìÅ Contents: ['part-00000-6b64f6cd-5f77-487e-bf7f-d3220977a1b6-c000.snappy.parquet', '.part-00000-6b64f6cd-5f77-487e-bf7f-d3220977a1b6-c000.snappy.parquet.crc', 'part-00010-6b64f6cd-5f77-487e-bf7f-d3220977a1b6-c000.snappy.parquet', 'part-00003-6b64f6cd-5f77-487e-bf7f-d3220977a1b6-c000.snappy.parquet', 'part-00007-6b64f6cd-5f77-487e-bf7f-d3220977a1b6-c000.snappy.parquet']...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === RELOAD DATA ===\n",
        "print(\"Loading data...\")\n",
        "df = spark.read.parquet(DATA_PATH)\n",
        "\n",
        "df_train = df.select(\n",
        "    'features_scaled',\n",
        "    'binary_label',\n",
        "    'unified_label',\n",
        "    'sample_weight',\n",
        "    'multiclass_weight'\n",
        ")\n",
        "\n",
        "# Split\n",
        "train_df, test_df = df_train.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# Sample (same fraction as before)\n",
        "SAMPLE_FRACTION = 0.15\n",
        "train_sampled = train_df.sample(fraction=SAMPLE_FRACTION, seed=42).cache()\n",
        "test_sampled = test_df.sample(fraction=SAMPLE_FRACTION, seed=42).cache()\n",
        "\n",
        "train_count = train_sampled.count()\n",
        "test_count = test_sampled.count()\n",
        "\n",
        "print(f\"‚úÖ Training set: {train_count:,} samples\")\n",
        "print(f\"‚úÖ Test set: {test_count:,} samples\")\n",
        "\n",
        "# Clean up\n",
        "df.unpersist()\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GByOiuSGuRkK",
        "outputId": "96695383-bb04-450c-f47e-9c415bced0c9"
      },
      "id": "GByOiuSGuRkK",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "‚úÖ Training set: 2,191,961 samples\n",
            "‚úÖ Test set: 548,492 samples\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "576"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === DIAGNOSTIC: Check class imbalance ===\n",
        "print(\"=\"*60)\n",
        "print(\"CLASS DISTRIBUTION ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\nüìä Training Set Class Distribution:\")\n",
        "train_sampled.groupBy('unified_label').count().orderBy('unified_label').show()\n",
        "\n",
        "print(\"\\nüìä Test Set Class Distribution:\")\n",
        "test_sampled.groupBy('unified_label').count().orderBy('unified_label').show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSqvUu29uoBz",
        "outputId": "7fe0c0e4-189f-42bb-cbba-9bfb0bb30456"
      },
      "id": "SSqvUu29uoBz",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "CLASS DISTRIBUTION ANALYSIS\n",
            "============================================================\n",
            "\n",
            "üìä Training Set Class Distribution:\n",
            "+-------------+-------+\n",
            "|unified_label|  count|\n",
            "+-------------+-------+\n",
            "|            0|1859701|\n",
            "|            1|  83895|\n",
            "|            2|  84335|\n",
            "|            3|  19958|\n",
            "|            4|    122|\n",
            "|            5|  19264|\n",
            "|            6|  33676|\n",
            "|            7|  10962|\n",
            "|            8|  80048|\n",
            "+-------------+-------+\n",
            "\n",
            "\n",
            "üìä Test Set Class Distribution:\n",
            "+-------------+------+\n",
            "|unified_label| count|\n",
            "+-------------+------+\n",
            "|            0|464986|\n",
            "|            1| 21029|\n",
            "|            2| 21303|\n",
            "|            3|  4933|\n",
            "|            4|    30|\n",
            "|            5|  4867|\n",
            "|            6|  8539|\n",
            "|            7|  2686|\n",
            "|            8| 20119|\n",
            "+-------------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === IMPROVED CLASS WEIGHTS ===\n",
        "print(\"=\"*60)\n",
        "print(\"Calculating improved class weights...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class_counts = train_sampled.groupBy('unified_label').count().collect()\n",
        "total_samples = train_sampled.count()\n",
        "num_classes = len(class_counts)\n",
        "\n",
        "# Inverse frequency with sqrt smoothing\n",
        "class_weight_map = {}\n",
        "for row in class_counts:\n",
        "    label = row['unified_label']\n",
        "    count = row['count']\n",
        "    weight = (total_samples / (num_classes * count)) ** 0.5\n",
        "    class_weight_map[label] = weight\n",
        "    print(f\"  Class {label}: {count:,} samples -> weight {weight:.4f}\")\n",
        "\n",
        "# Apply weights\n",
        "from pyspark.sql.functions import when, col, lit\n",
        "\n",
        "weight_expr = lit(1.0)\n",
        "for label, weight in class_weight_map.items():\n",
        "    weight_expr = when(col('unified_label') == label, weight).otherwise(weight_expr)\n",
        "\n",
        "train_reweighted = train_sampled.withColumn('improved_weight', weight_expr)\n",
        "train_reweighted.cache()\n",
        "train_reweighted.count()\n",
        "\n",
        "print(\"\\n‚úÖ Improved weights applied\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXczUvrCuvKK",
        "outputId": "8563a187-be42-4c0d-956d-66dc4791d6e3"
      },
      "id": "GXczUvrCuvKK",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Calculating improved class weights...\n",
            "============================================================\n",
            "  Class 8: 80,048 samples -> weight 1.7443\n",
            "  Class 7: 10,962 samples -> weight 4.7136\n",
            "  Class 1: 83,895 samples -> weight 1.7038\n",
            "  Class 6: 33,676 samples -> weight 2.6893\n",
            "  Class 3: 19,958 samples -> weight 3.4933\n",
            "  Class 5: 19,264 samples -> weight 3.5557\n",
            "  Class 2: 84,335 samples -> weight 1.6994\n",
            "  Class 0: 1,859,701 samples -> weight 0.3619\n",
            "  Class 4: 122 samples -> weight 44.6802\n",
            "\n",
            "‚úÖ Improved weights applied\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === TRAIN IMPROVED MULTI-CLASS MODEL ===\n",
        "print(\"=\"*60)\n",
        "print(\"Training IMPROVED Random Forest - Multi-class\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "rf_multi_improved = RandomForestClassifier(\n",
        "    featuresCol='features_scaled',\n",
        "    labelCol='unified_label',\n",
        "    weightCol='improved_weight',\n",
        "    numTrees=50,\n",
        "    maxDepth=12,\n",
        "    maxBins=64,\n",
        "    minInstancesPerNode=5,\n",
        "    minInfoGain=0.0,\n",
        "    featureSubsetStrategy='sqrt',\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "print(\"üöÄ Training...\")\n",
        "rf_multi_improved_model = rf_multi_improved.fit(train_reweighted)\n",
        "elapsed = time.time() - start_time\n",
        "print(f\"‚úÖ Training completed in {elapsed/60:.2f} minutes\")\n",
        "\n",
        "train_reweighted.unpersist()\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQrRPag2v7BL",
        "outputId": "574eac79-4f9d-4330-d38e-26fd63430d6c"
      },
      "id": "tQrRPag2v7BL",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Training IMPROVED Random Forest - Multi-class\n",
            "============================================================\n",
            "üöÄ Training...\n",
            "‚úÖ Training completed in 3.78 minutes\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "260"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === EVALUATE IMPROVED MODEL ===\n",
        "print(\"=\"*60)\n",
        "print(\"Evaluating Improved Multi-class Model\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "preds = rf_multi_improved_model.transform(test_sampled)\n",
        "\n",
        "mc_evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol='unified_label',\n",
        "    predictionCol='prediction'\n",
        ")\n",
        "\n",
        "accuracy = mc_evaluator.evaluate(preds, {mc_evaluator.metricName: 'accuracy'})\n",
        "f1 = mc_evaluator.evaluate(preds, {mc_evaluator.metricName: 'f1'})\n",
        "precision = mc_evaluator.evaluate(preds, {mc_evaluator.metricName: 'weightedPrecision'})\n",
        "recall = mc_evaluator.evaluate(preds, {mc_evaluator.metricName: 'weightedRecall'})\n",
        "\n",
        "print(f\"\\nAccuracy:  {accuracy:.4f}\")\n",
        "print(f\"F1 Score:  {f1:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall:    {recall:.4f}\")\n",
        "\n",
        "# Compare with original (0.5573 accuracy, 0.6813 F1, 0.5573 recall)\n",
        "print(\"\\nüìä IMPROVEMENT vs ORIGINAL:\")\n",
        "print(f\"  Accuracy: 0.5573 -> {accuracy:.4f} ({(accuracy-0.5573)*100:+.2f}%)\")\n",
        "print(f\"  F1 Score: 0.6813 -> {f1:.4f} ({(f1-0.6813)*100:+.2f}%)\")\n",
        "print(f\"  Recall:   0.5573 -> {recall:.4f} ({(recall-0.5573)*100:+.2f}%)\")\n",
        "\n",
        "# Per-class recall\n",
        "print(\"\\nüìä Per-Class Recall:\")\n",
        "preds.withColumn('correct', F.when(F.col('unified_label') == F.col('prediction'), 1).otherwise(0)) \\\n",
        "    .groupBy('unified_label').agg(\n",
        "        F.count('*').alias('total'),\n",
        "        F.sum('correct').alias('correct'),\n",
        "        F.round(F.sum('correct') / F.count('*'), 4).alias('recall')\n",
        "    ).orderBy('unified_label').show()\n",
        "\n",
        "preds.unpersist()\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6bQXtApwBtr",
        "outputId": "c9e0f1bc-35c0-49b9-94ce-7f2b6154a344"
      },
      "id": "w6bQXtApwBtr",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Evaluating Improved Multi-class Model\n",
            "============================================================\n",
            "\n",
            "Accuracy:  0.9870\n",
            "F1 Score:  0.9858\n",
            "Precision: 0.9851\n",
            "Recall:    0.9870\n",
            "\n",
            "üìä IMPROVEMENT vs ORIGINAL:\n",
            "  Accuracy: 0.5573 -> 0.9870 (+42.97%)\n",
            "  F1 Score: 0.6813 -> 0.9858 (+30.45%)\n",
            "  Recall:   0.5573 -> 0.9870 (+42.97%)\n",
            "\n",
            "üìä Per-Class Recall:\n",
            "+-------------+------+-------+------+\n",
            "|unified_label| total|correct|recall|\n",
            "+-------------+------+-------+------+\n",
            "|            0|464986| 462475|0.9946|\n",
            "|            1| 21029|  20343|0.9674|\n",
            "|            2| 21303|  21278|0.9988|\n",
            "|            3|  4933|   4924|0.9982|\n",
            "|            4|    30|     18|   0.6|\n",
            "|            5|  4867|   1091|0.2242|\n",
            "|            6|  8539|   8500|0.9954|\n",
            "|            7|  2686|   2681|0.9981|\n",
            "|            8| 20119|  20035|0.9958|\n",
            "+-------------+------+-------+------+\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "199"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# === SAVE IMPROVED MODEL ===\n",
        "improved_model_path = f\"{MODEL_DIR}/rf_multiclass_improved\"\n",
        "rf_multi_improved_model.write().overwrite().save(improved_model_path)\n",
        "print(f\"‚úÖ Improved model saved to: {improved_model_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzXmkvFTwGqz",
        "outputId": "6727e1c8-d91e-4b34-e739-981aa27065ca"
      },
      "id": "wzXmkvFTwGqz",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Improved model saved to: /content/drive/MyDrive/NetworkIDS/output/models/rf_multiclass_improved\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V5E1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}